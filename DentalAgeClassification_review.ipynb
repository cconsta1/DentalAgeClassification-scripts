{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "https://github.com/cconsta1/DentalAgeClassification-scripts/blob/main/DentalAgeClassification_review.ipynb",
      "authorship_tag": "ABX9TyMIHkNdMgRdLkEYisnzWdO6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cconsta1/DentalAgeClassification-scripts/blob/main/DentalAgeClassification_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Data Loading and Setup\n",
        "# -----------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import re\n",
        "from scipy import stats\n",
        "import json\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/DentAgePooledDatav2.zip'\n",
        "local_zip = './DentAgePooledDatav2.zip'\n",
        "local_extract = '.'\n",
        "\n",
        "\n",
        "if not os.path.exists(local_zip):\n",
        "    print(\"Copying ZIP from Drive to Colab local disk (safe copy)...\")\n",
        "    with open(zip_path, 'rb') as src, open(local_zip, 'wb') as dst:\n",
        "        shutil.copyfileobj(src, dst, length=1024*1024)  # 1 MB buffer\n",
        "    print(\"Copy complete.\")\n",
        "else:\n",
        "    print(\"Local ZIP already exists.\")\n",
        "\n",
        "print(f\"Drive ZIP size: {os.path.getsize(zip_path)}\")\n",
        "print(f\"Local ZIP size: {os.path.getsize(local_zip)}\")\n",
        "\n",
        "if not os.path.exists('./DentAgePooledDatav2'):\n",
        "    print(\"Extracting ZIP...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(local_zip, 'r') as zip_ref:\n",
        "            zip_ref.extractall(local_extract)\n",
        "        print(\"Extraction complete.\")\n",
        "    except zipfile.BadZipFile:\n",
        "        raise RuntimeError(\"ERROR: The copied ZIP is corrupted. Delete it and rerun the copy step.\")\n",
        "else:\n",
        "    print(\"Dataset already extracted.\")\n",
        "\n",
        "DATASET_PATH = './DentAgePooledDatav2'\n",
        "print(f\"Dataset path: {DATASET_PATH}\")\n"
      ],
      "metadata": {
        "id": "r5QZTZGnowyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metadata extraction and DataFrame creation\n",
        "def extract_metadata_from_filename(filepath):\n",
        "    \"\"\"Extract age, gender from filename and folder\"\"\"\n",
        "    filename = os.path.basename(filepath)\n",
        "    folder_name = os.path.basename(os.path.dirname(filepath))\n",
        "\n",
        "    # Extract age from folder name (e.g., \"14Y\" -> 14)\n",
        "    age_match = re.search(r'(\\d+)Y', folder_name)\n",
        "    age = int(age_match.group(1)) if age_match else None\n",
        "\n",
        "    # Extract gender - Look for M or F in filename (case insensitive)\n",
        "    gender = None\n",
        "    filename_upper = filename.upper()\n",
        "    if 'F' in filename_upper:\n",
        "        gender = 'F'\n",
        "    elif 'M' in filename_upper:\n",
        "        gender = 'M'\n",
        "\n",
        "    return {\n",
        "        'filepath': filepath,\n",
        "        'filename': filename,\n",
        "        'age': age,\n",
        "        'gender': gender,\n",
        "        'file_extension': os.path.splitext(filename)[1]\n",
        "    }\n",
        "\n",
        "def create_dataset_dataframe(dataset_path):\n",
        "    \"\"\"Create DataFrame with ages 14-24 only\"\"\"\n",
        "    data_list = []\n",
        "\n",
        "    # Get all image files\n",
        "    image_extensions = ['*.jpg', '*.jpeg', '*.bmp', '*.png',\n",
        "                       '*.JPG', '*.JPEG', '*.BMP', '*.PNG']\n",
        "    all_files = []\n",
        "\n",
        "    for ext in image_extensions:\n",
        "        all_files.extend(glob.glob(os.path.join(dataset_path, '**', ext), recursive=True))\n",
        "\n",
        "    print(f\"Found {len(all_files)} total image files\")\n",
        "\n",
        "    # Remove duplicates\n",
        "    all_files = list(set(all_files))\n",
        "    print(f\"After removing duplicates: {len(all_files)} files\")\n",
        "\n",
        "    for filepath in all_files:\n",
        "        metadata = extract_metadata_from_filename(filepath)\n",
        "\n",
        "        # Only process files in age range 14-24 with valid gender\n",
        "        if (metadata['age'] is not None and 14 <= metadata['age'] <= 24\n",
        "            and metadata['gender'] is not None):\n",
        "            data_list.append(metadata)\n",
        "\n",
        "    print(f\"Files with valid age (14-24) and gender: {len(data_list)}\")\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "\n",
        "    if len(df) > 0:\n",
        "        # Age distribution\n",
        "        age_counts = df['age'].value_counts().sort_index()\n",
        "        print(f\"\\nAge distribution:\")\n",
        "        for age in sorted(age_counts.index):\n",
        "            print(f\"Age {age}: {age_counts[age]} samples\")\n",
        "\n",
        "        # Gender distribution\n",
        "        gender_counts = df['gender'].value_counts()\n",
        "        print(f\"\\nGender distribution:\")\n",
        "        for gender, count in gender_counts.items():\n",
        "            print(f\"{gender}: {count} samples\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Create the dataset DataFrame\n",
        "print(\"Creating dataset DataFrame (ages 14-24 years)...\")\n",
        "df_fixed = create_dataset_dataframe(DATASET_PATH)\n",
        "\n",
        "print(f\"\\nDataset Summary:\")\n",
        "print(f\"Total samples: {len(df_fixed)}\")\n",
        "print(f\"DataFrame shape: {df_fixed.shape}\")\n",
        "print(f\"Columns: {list(df_fixed.columns)}\")\n",
        "\n",
        "if len(df_fixed) > 0:\n",
        "    print(f\"Age range: {df_fixed['age'].min()} - {df_fixed['age'].max()} years\")\n",
        "\n",
        "    # Add binary label (minor/adult) and normalized age\n",
        "    age_min, age_max = df_fixed['age'].min(), df_fixed['age'].max()\n",
        "    df_fixed['bin_label'] = (df_fixed['age'] >= 18).astype(int)\n",
        "    df_fixed['age_norm'] = (df_fixed['age'] - age_min) / (age_max - age_min)\n",
        "\n",
        "    # Histogram of age distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(df_fixed['age'], bins=range(14, 26), alpha=0.7, rwidth=0.8)\n",
        "    plt.axvline(x=18, color='red', linestyle='--', label='Adult threshold (18 years)')\n",
        "    plt.xlabel('Age (years)')\n",
        "    plt.ylabel('Number of samples')\n",
        "    plt.title('Age Distribution in Dataset')\n",
        "    plt.xticks(range(14, 25))\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No valid samples found!\")"
      ],
      "metadata": {
        "id": "HVnenrqJowt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation Visualization\n",
        "def visualize_transformations(image_path):\n",
        "    \"\"\"Visualize various transformations applied to a dental panoramic image\"\"\"\n",
        "    original = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Define transformations\n",
        "    transform_list = {\n",
        "        'Original': transforms.Compose([\n",
        "            transforms.Resize((384, 384)),\n",
        "            transforms.ToTensor(),\n",
        "        ]),\n",
        "        'Normalized': transforms.Compose([\n",
        "            transforms.Resize((384, 384)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'Horizontal Flip': transforms.Compose([\n",
        "            transforms.Resize((384, 384)),\n",
        "            transforms.RandomHorizontalFlip(p=1.0),\n",
        "            transforms.ToTensor(),\n",
        "        ]),\n",
        "        'Rotation': transforms.Compose([\n",
        "            transforms.Resize((384, 384)),\n",
        "            transforms.RandomRotation(degrees=10),\n",
        "            transforms.ToTensor(),\n",
        "        ]),\n",
        "        'Color Jitter': transforms.Compose([\n",
        "            transforms.Resize((384, 384)),\n",
        "            transforms.ColorJitter(brightness=0.15, contrast=0.15),\n",
        "            transforms.ToTensor(),\n",
        "        ]),\n",
        "        'Random Erasing': transforms.Compose([\n",
        "            transforms.Resize((384, 384)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomErasing(p=1.0, scale=(0.02, 0.1))\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    # Apply transformations and visualize\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    # Original image\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.imshow(original)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Transformed images\n",
        "    for i, (name, transform) in enumerate(list(transform_list.items())[1:], start=2):\n",
        "        img_tensor = transform(original)\n",
        "\n",
        "        # For display\n",
        "        if 'Normalized' in name:\n",
        "            # Denormalize for display\n",
        "            img_np = img_tensor.numpy().transpose(1, 2, 0)\n",
        "            img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "            img_np = np.clip(img_np, 0, 1)\n",
        "        else:\n",
        "            img_np = img_tensor.numpy().transpose(1, 2, 0)\n",
        "\n",
        "        plt.subplot(2, 3, i)\n",
        "        plt.imshow(img_np)\n",
        "        plt.title(name)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle('Dental X-ray Image Preprocessing Pipeline', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('transformation_visualization.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return \"Image transformation pipeline visualized\"\n",
        "\n"
      ],
      "metadata": {
        "id": "nF9PBdTfowmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample one image from each age group\n",
        "sample_image = df_fixed.iloc[48]['filepath']\n",
        "visualize_transformations(sample_image)"
      ],
      "metadata": {
        "id": "r4kUxttMgluH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed get_transforms function for backbone compatibility\n",
        "def get_transforms(augment=False, backbone='efficientnet'):\n",
        "    \"\"\"Get training and validation transforms with optional augmentation\"\"\"\n",
        "    # ViT requires 224x224 input, EfficientNet can work with 384x384\n",
        "    img_size = 224 if backbone == 'vit' else 384\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5) if augment else transforms.Lambda(lambda x: x),\n",
        "        transforms.RandomRotation(degrees=10) if augment else transforms.Lambda(lambda x: x),\n",
        "        transforms.ColorJitter(brightness=0.15, contrast=0.15) if augment else transforms.Lambda(lambda x: x),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)) if augment else transforms.Lambda(lambda x: x)\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# Age to class mapping\n",
        "age_min, age_max = 14, 24\n",
        "age2class = {age: idx for idx, age in enumerate(range(age_min, age_max+1))}\n",
        "class2age = {v: k for k, v in age2class.items()}\n",
        "n_classes = len(age2class)\n",
        "\n",
        "def normalize_age(age):\n",
        "    \"\"\"Normalize age to [0, 1] range\"\"\"\n",
        "    return (age - age_min) / (age_max - age_min)\n",
        "\n",
        "def denormalize_age(norm_age):\n",
        "    \"\"\"Denormalize age from [0, 1] range\"\"\"\n",
        "    return norm_age * (age_max - age_min) + age_min"
      ],
      "metadata": {
        "id": "_xAiD0bEowgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flexible Dataset class for all task types\n",
        "class DentalAgeDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform, task_type='binary'):\n",
        "        \"\"\"\n",
        "        task_type: 'binary' (adult/minor), 'multiclass' (per-year), or 'regression'\n",
        "        \"\"\"\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.task_type = task_type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row['filepath']).convert('L')  # Convert to grayscale\n",
        "        img = img.convert('RGB')  # Convert back to RGB for model compatibility\n",
        "        img = self.transform(img)\n",
        "\n",
        "        if self.task_type == 'binary':\n",
        "            # Binary classification: 0 for minor, 1 for adult\n",
        "            label = 1 if int(row['age']) >= 18 else 0\n",
        "        elif self.task_type == 'multiclass':\n",
        "            # Multiclass classification: map age to class index\n",
        "            label = age2class[int(row['age'])]\n",
        "        elif self.task_type == 'regression':\n",
        "            # Regression: normalize age to [0, 1]\n",
        "            label = np.float32(row['age_norm'])\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task type: {self.task_type}\")\n",
        "\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "g0W-WMDnqPIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flexible Neural Network Architecture\n",
        "class DentalAgeModel(nn.Module):\n",
        "    def __init__(self, task_type='binary', backbone='efficientnet', use_attention=True):\n",
        "        \"\"\"\n",
        "        Initialize a model for dental age estimation.\n",
        "\n",
        "        Args:\n",
        "            task_type: 'binary' (adult/minor), 'multiclass' (per-year), or 'regression'\n",
        "            backbone: 'efficientnet' or 'vit'\n",
        "            use_attention: Whether to use self-attention mechanism\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.task_type = task_type\n",
        "        self.backbone_type = backbone\n",
        "        self.use_attention = use_attention\n",
        "\n",
        "        # Feature extractor\n",
        "        if backbone == 'efficientnet':\n",
        "            self.backbone = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "            self.backbone.classifier = nn.Identity()\n",
        "            self.feature_dim = 1280\n",
        "        elif backbone == 'vit':\n",
        "            self.backbone = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
        "            self.backbone.heads = nn.Identity()\n",
        "            self.feature_dim = 768\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
        "\n",
        "        # Self-attention mechanism\n",
        "        if use_attention:\n",
        "            self.attention = SelfAttention(self.feature_dim)\n",
        "\n",
        "        # Task-specific layers\n",
        "        if task_type in ['binary', 'regression']:\n",
        "            # Binary classification or regression\n",
        "            self.fc1 = nn.Linear(self.feature_dim, 256)\n",
        "            self.bn1 = nn.BatchNorm1d(256)\n",
        "            self.dropout1 = nn.Dropout(0.2)\n",
        "            self.fc2 = nn.Linear(256, 128)\n",
        "            self.bn2 = nn.BatchNorm1d(128)\n",
        "            self.dropout2 = nn.Dropout(0.2)\n",
        "            self.fc3 = nn.Linear(128, 1)\n",
        "        elif task_type == 'multiclass':\n",
        "            # Multiclass classification\n",
        "            self.fc1 = nn.Linear(self.feature_dim, 512)\n",
        "            self.bn1 = nn.BatchNorm1d(512)\n",
        "            self.dropout1 = nn.Dropout(0.25)\n",
        "            self.fc2 = nn.Linear(512, 256)\n",
        "            self.bn2 = nn.BatchNorm1d(256)\n",
        "            self.dropout2 = nn.Dropout(0.25)\n",
        "            self.fc3 = nn.Linear(256, 128)\n",
        "            self.bn3 = nn.BatchNorm1d(128)\n",
        "            self.dropout3 = nn.Dropout(0.25)\n",
        "            self.fc4 = nn.Linear(128, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Apply attention if specified\n",
        "        if self.use_attention:\n",
        "            features = self.attention(features)\n",
        "\n",
        "        # Task-specific forward pass\n",
        "        if self.task_type == 'multiclass':\n",
        "            x = self.fc1(features)\n",
        "            x = self.bn1(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout1(x)\n",
        "\n",
        "            x = self.fc2(x)\n",
        "            x = self.bn2(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout2(x)\n",
        "\n",
        "            x = self.fc3(x)\n",
        "            x = self.bn3(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout3(x)\n",
        "\n",
        "            x = self.fc4(x)\n",
        "            return x  # No activation for multiclass (will use with CrossEntropyLoss)\n",
        "        else:\n",
        "            # For binary and regression\n",
        "            x = self.fc1(features)\n",
        "            x = self.bn1(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout1(x)\n",
        "\n",
        "            x = self.fc2(x)\n",
        "            x = self.bn2(x)\n",
        "            x = F.relu(x)\n",
        "            x = self.dropout2(x)\n",
        "\n",
        "            x = self.fc3(x).squeeze(1)\n",
        "\n",
        "            # Apply sigmoid for binary classification and regression\n",
        "            if self.task_type in ['binary', 'regression']:\n",
        "                x = torch.sigmoid(x)\n",
        "\n",
        "            return x\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.query(x).unsqueeze(1)  # (batch, 1, hidden_dim)\n",
        "        k = self.key(x).unsqueeze(1)    # (batch, 1, hidden_dim)\n",
        "        v = self.value(x).unsqueeze(1)  # (batch, 1, hidden_dim)\n",
        "        attn_weights = self.softmax(torch.bmm(q, k.transpose(1,2)) / (x.size(1) ** 0.5))\n",
        "        attn_output = torch.bmm(attn_weights, v)\n",
        "        return attn_output.squeeze(1)"
      ],
      "metadata": {
        "id": "buC7cIbyqPCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix prepare_data function to handle different backbones\n",
        "def prepare_data(df, task_type='binary', augment=False, sex_filter=None, backbone='efficientnet'):\n",
        "    \"\"\"\n",
        "    Prepare data for model training.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with dataset\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "        augment: Whether to apply data augmentation\n",
        "        sex_filter: None for all, 'M' for males only, 'F' for females only\n",
        "        backbone: 'efficientnet' or 'vit'\n",
        "\n",
        "    Returns:\n",
        "        train_loader, val_loader, train_df, val_df\n",
        "    \"\"\"\n",
        "    # Filter by sex if needed\n",
        "    if sex_filter in ['M', 'F']:\n",
        "        df = df[df['gender'] == sex_filter].reset_index(drop=True)\n",
        "        print(f\"Using {sex_filter} only data: {len(df)} samples\")\n",
        "\n",
        "    # Get transforms with appropriate size for backbone\n",
        "    train_transform, val_transform = get_transforms(augment, backbone)\n",
        "\n",
        "    # Prepare for stratification\n",
        "    if task_type == 'binary':\n",
        "        stratify_col = 'bin_label'\n",
        "    else:\n",
        "        # For multiclass and regression, stratify by age\n",
        "        stratify_col = 'age'\n",
        "\n",
        "    # Split data\n",
        "    train_df, val_df = train_test_split(\n",
        "        df, test_size=0.2,\n",
        "        stratify=df[stratify_col],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = DentalAgeDataset(train_df, train_transform, task_type)\n",
        "    val_dataset = DentalAgeDataset(val_df, val_transform, task_type)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 32 if task_type == 'regression' else 16\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Data augmentation (if needed) - duplicate training data with transforms\n",
        "    if augment:\n",
        "        # Create augmented version of training data\n",
        "        train_aug_dataset = DentalAgeDataset(train_df, train_transform, task_type)\n",
        "        train_combined = torch.utils.data.ConcatDataset([train_dataset, train_aug_dataset])\n",
        "        train_loader = DataLoader(train_combined, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        print(f\"Augmented train samples: {len(train_combined)}\")\n",
        "\n",
        "    return train_loader, val_loader, train_df, val_df"
      ],
      "metadata": {
        "id": "LiEpEsowqO7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Universal training function\n",
        "def train_model(model, train_loader, val_loader, task_type='binary', num_epochs=25):\n",
        "    \"\"\"\n",
        "    Train a model for any of the task types.\n",
        "\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        train_loader, val_loader: Data loaders\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "        num_epochs: Number of epochs to train\n",
        "\n",
        "    Returns:\n",
        "        Trained model, loss history, performance history\n",
        "    \"\"\"\n",
        "    # Set up loss function and optimizer\n",
        "    if task_type == 'binary':\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "    elif task_type == 'multiclass':\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    elif task_type == 'regression':\n",
        "        criterion = nn.MSELoss()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown task type: {task_type}\")\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=5, factor=0.5\n",
        "    )\n",
        "\n",
        "    # Training history\n",
        "    train_losses, val_losses = [], []\n",
        "    performance_history = []\n",
        "    best_metric = float('inf') if task_type == 'regression' else 0\n",
        "    patience, patience_counter = 7, 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            if task_type == 'binary':\n",
        "                # Binary classification needs logits, but our model outputs sigmoids\n",
        "                loss = criterion(outputs, labels.float())\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds, val_targets = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "\n",
        "                if task_type == 'binary':\n",
        "                    loss = criterion(outputs, labels.float())\n",
        "                    preds = (outputs >= 0.5).float()\n",
        "                elif task_type == 'multiclass':\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    preds = torch.argmax(outputs, dim=1)\n",
        "                else:  # regression\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    preds = outputs\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        if task_type == 'binary':\n",
        "            acc = accuracy_score(val_targets, val_preds)\n",
        "            performance_metric = acc\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - Val Acc: {acc:.4f}\")\n",
        "        elif task_type == 'multiclass':\n",
        "            acc = accuracy_score(val_targets, val_preds)\n",
        "            performance_metric = acc\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - Val Acc: {acc:.4f}\")\n",
        "        else:  # regression\n",
        "            # Convert predictions back to years\n",
        "            val_pred_years = [denormalize_age(x) for x in val_preds]\n",
        "            val_true_years = [denormalize_age(x) for x in val_targets]\n",
        "            mae = mean_absolute_error(val_true_years, val_pred_years)\n",
        "            performance_metric = mae  # Lower is better for MAE\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - Val MAE: {mae:.3f}\")\n",
        "\n",
        "        performance_history.append(performance_metric)\n",
        "\n",
        "        # Early stopping and model saving\n",
        "        improved = False\n",
        "        if task_type == 'regression':\n",
        "            # For regression, lower MAE is better\n",
        "            if performance_metric < best_metric:\n",
        "                best_metric = performance_metric\n",
        "                improved = True\n",
        "        else:\n",
        "            # For classification, higher accuracy is better\n",
        "            if performance_metric > best_metric:\n",
        "                best_metric = performance_metric\n",
        "                improved = True\n",
        "\n",
        "        if improved:\n",
        "            patience_counter = 0\n",
        "            print(f\"New best model with {'MAE' if task_type == 'regression' else 'accuracy'}: {best_metric:.4f}\")\n",
        "\n",
        "            # Save the model\n",
        "            MODEL_DIR = f\"/content/drive/MyDrive/models/DentalAge_{task_type}_{model.backbone_type}\"\n",
        "            os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"{MODEL_DIR}/best_model.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    metric_name = 'MAE (years)' if task_type == 'regression' else 'Accuracy'\n",
        "    plt.plot(performance_history, color='green')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.title(f'Validation {metric_name}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{MODEL_DIR}/training_curves.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Load best model for evaluation\n",
        "    model.load_state_dict(torch.load(f\"{MODEL_DIR}/best_model.pth\"))\n",
        "\n",
        "    return model, train_losses, val_losses, performance_history"
      ],
      "metadata": {
        "id": "6q015gKKqOzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed Comprehensive evaluation function\n",
        "def evaluate_model(model, val_loader, task_type='binary'):\n",
        "    \"\"\"\n",
        "    Evaluate a model with comprehensive metrics.\n",
        "\n",
        "    Args:\n",
        "        model: The model to evaluate\n",
        "        val_loader: Validation data loader\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of performance metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    all_outputs = []  # Store raw outputs for ROC calculation\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            # Store raw outputs for later use\n",
        "            all_outputs.extend(outputs.cpu().numpy())\n",
        "\n",
        "            if task_type == 'binary':\n",
        "                preds = (outputs >= 0.5).float()\n",
        "            elif task_type == 'multiclass':\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "            else:  # regression\n",
        "                preds = outputs\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "    all_outputs = np.array(all_outputs)\n",
        "\n",
        "    # Task-specific evaluation\n",
        "    if task_type == 'binary':\n",
        "        # Binary classification metrics\n",
        "        acc = accuracy_score(all_targets, all_preds)\n",
        "        precision = precision_score(all_targets, all_preds)\n",
        "        recall = sensitivity = recall_score(all_targets, all_preds)\n",
        "        specificity = recall_score(1-all_targets, 1-all_preds)\n",
        "        f1 = f1_score(all_targets, all_preds)\n",
        "\n",
        "        # Calculate confidence intervals (95%) using bootstrapping\n",
        "        n_bootstraps = 1000\n",
        "        rng = np.random.RandomState(42)\n",
        "\n",
        "        bootstrap_acc = []\n",
        "        bootstrap_sensitivity = []\n",
        "        bootstrap_specificity = []\n",
        "\n",
        "        for _ in range(n_bootstraps):\n",
        "            indices = rng.randint(0, len(all_preds), len(all_preds))\n",
        "            if len(np.unique(all_targets[indices])) < 2:\n",
        "                continue  # Skip if only one class is present\n",
        "\n",
        "            boot_acc = accuracy_score(all_targets[indices], all_preds[indices])\n",
        "            boot_sens = recall_score(all_targets[indices], all_preds[indices], zero_division=0)\n",
        "            boot_spec = recall_score(1-all_targets[indices], 1-all_preds[indices], zero_division=0)\n",
        "\n",
        "            bootstrap_acc.append(boot_acc)\n",
        "            bootstrap_sensitivity.append(boot_sens)\n",
        "            bootstrap_specificity.append(boot_spec)\n",
        "\n",
        "        # Calculate confidence intervals\n",
        "        alpha = 0.95\n",
        "        ci_acc = np.percentile(bootstrap_acc, [(1-alpha)/2 * 100, (alpha + (1-alpha)/2) * 100])\n",
        "        ci_sens = np.percentile(bootstrap_sensitivity, [(1-alpha)/2 * 100, (alpha + (1-alpha)/2) * 100])\n",
        "        ci_spec = np.percentile(bootstrap_specificity, [(1-alpha)/2 * 100, (alpha + (1-alpha)/2) * 100])\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        # ROC curve and AUC - using all_outputs instead of outputs\n",
        "        roc_auc = roc_auc_score(all_targets, all_outputs)\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': acc,\n",
        "            'accuracy_ci': ci_acc,\n",
        "            'sensitivity': sensitivity,\n",
        "            'sensitivity_ci': ci_sens,\n",
        "            'specificity': specificity,\n",
        "            'specificity_ci': ci_spec,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'roc_auc': roc_auc,\n",
        "            'confusion_matrix': cm,\n",
        "            'predictions': all_preds,\n",
        "            'targets': all_targets,\n",
        "            'probabilities': all_outputs\n",
        "        }\n",
        "\n",
        "        print(\"\\nBinary Classification Results:\")\n",
        "        print(f\"Accuracy: {acc:.4f} (95% CI: {ci_acc[0]:.4f}-{ci_acc[1]:.4f})\")\n",
        "        print(f\"Sensitivity (TPR): {sensitivity:.4f} (95% CI: {ci_sens[0]:.4f}-{ci_sens[1]:.4f})\")\n",
        "        print(f\"Specificity (TNR): {specificity:.4f} (95% CI: {ci_spec[0]:.4f}-{ci_spec[1]:.4f})\")\n",
        "        print(f\"Precision (PPV): {precision:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(f\"TN: {tn}, FP: {fp}\")\n",
        "        print(f\"FN: {fn}, TP: {tp}\")\n",
        "\n",
        "    elif task_type == 'multiclass':\n",
        "        # Convert class indices to ages\n",
        "        all_pred_ages = np.array([class2age[x] for x in all_preds])\n",
        "        all_true_ages = np.array([class2age[x] for x in all_targets])\n",
        "\n",
        "        # Classification metrics\n",
        "        acc = accuracy_score(all_targets, all_preds)\n",
        "        mae = mean_absolute_error(all_true_ages, all_pred_ages)\n",
        "        rmse = np.sqrt(mean_squared_error(all_true_ages, all_pred_ages))\n",
        "\n",
        "        # One-vs-rest classification metrics for each age class\n",
        "        class_report = classification_report(all_targets, all_preds,\n",
        "                                           target_names=[str(class2age[i]) for i in range(n_classes)],\n",
        "                                           output_dict=True)\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': acc,\n",
        "            'mae': mae,\n",
        "            'rmse': rmse,\n",
        "            'class_report': class_report,\n",
        "            'confusion_matrix': cm,\n",
        "            'predictions': all_preds,\n",
        "            'targets': all_targets\n",
        "        }\n",
        "\n",
        "        print(\"\\nMulticlass Classification Results:\")\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "        print(f\"MAE (years): {mae:.3f}\")\n",
        "        print(f\"RMSE (years): {rmse:.3f}\")\n",
        "\n",
        "    else:  # regression\n",
        "        # Convert normalized predictions back to years\n",
        "        all_pred_years = np.array([denormalize_age(x) for x in all_preds])\n",
        "        all_true_years = np.array([denormalize_age(x) for x in all_targets])\n",
        "\n",
        "        # Regression metrics\n",
        "        mae = mean_absolute_error(all_true_years, all_pred_years)\n",
        "        rmse = np.sqrt(mean_squared_error(all_true_years, all_pred_years))\n",
        "        r2 = r2_score(all_true_years, all_pred_years)\n",
        "\n",
        "        # Per-age MAE\n",
        "        age_metrics = {}\n",
        "        for age in range(age_min, age_max + 1):\n",
        "            age_indices = [i for i, y in enumerate(all_true_years) if int(y) == age]\n",
        "            if age_indices:\n",
        "                age_true = all_true_years[age_indices]\n",
        "                age_pred = all_pred_years[age_indices]\n",
        "                age_mae = mean_absolute_error(age_true, age_pred)\n",
        "                within_1yr = np.mean(np.abs(age_pred - age_true) <= 1.0)\n",
        "                age_metrics[age] = {\n",
        "                    'mae': age_mae,\n",
        "                    'within_1yr': within_1yr,\n",
        "                    'samples': len(age_indices)\n",
        "                }\n",
        "\n",
        "        # Calculate binary adult/minor classification metrics using regression predictions\n",
        "        binary_preds = (all_pred_years >= 18).astype(int)\n",
        "        binary_targets = (all_true_years >= 18).astype(int)\n",
        "        binary_acc = accuracy_score(binary_targets, binary_preds)\n",
        "        sensitivity = recall_score(binary_targets, binary_preds)\n",
        "        specificity = recall_score(1-binary_targets, 1-binary_preds)\n",
        "\n",
        "        # Confidence intervals for regression metrics using bootstrapping\n",
        "        n_bootstraps = 1000\n",
        "        rng = np.random.RandomState(42)\n",
        "        bootstrap_mae = []\n",
        "        bootstrap_binary_acc = []\n",
        "\n",
        "        for _ in range(n_bootstraps):\n",
        "            indices = rng.randint(0, len(all_pred_years), len(all_pred_years))\n",
        "            boot_mae = mean_absolute_error(all_true_years[indices], all_pred_years[indices])\n",
        "            boot_acc = accuracy_score(binary_targets[indices], binary_preds[indices])\n",
        "            bootstrap_mae.append(boot_mae)\n",
        "            bootstrap_binary_acc.append(boot_acc)\n",
        "\n",
        "        # Calculate confidence intervals\n",
        "        alpha = 0.95\n",
        "        ci_mae = np.percentile(bootstrap_mae, [(1-alpha)/2 * 100, (alpha + (1-alpha)/2) * 100])\n",
        "        ci_binary_acc = np.percentile(bootstrap_binary_acc, [(1-alpha)/2 * 100, (alpha + (1-alpha)/2) * 100])\n",
        "\n",
        "        metrics = {\n",
        "            'mae': mae,\n",
        "            'mae_ci': ci_mae,\n",
        "            'rmse': rmse,\n",
        "            'r2': r2,\n",
        "            'binary_acc': binary_acc,\n",
        "            'binary_acc_ci': ci_binary_acc,\n",
        "            'sensitivity': sensitivity,\n",
        "            'specificity': specificity,\n",
        "            'age_metrics': age_metrics,\n",
        "            'predictions': all_pred_years,\n",
        "            'targets': all_true_years\n",
        "        }\n",
        "\n",
        "        print(\"\\nRegression Results:\")\n",
        "        print(f\"MAE (years): {mae:.3f} (95% CI: {ci_mae[0]:.3f}-{ci_mae[1]:.3f})\")\n",
        "        print(f\"RMSE (years): {rmse:.3f}\")\n",
        "        print(f\"R²: {r2:.3f}\")\n",
        "        print(f\"Binary classification accuracy: {binary_acc:.4f} (95% CI: {ci_binary_acc[0]:.4f}-{ci_binary_acc[1]:.4f})\")\n",
        "        print(f\"Sensitivity: {sensitivity:.4f}\")\n",
        "        print(f\"Specificity: {specificity:.4f}\")\n",
        "\n",
        "        print(\"\\nAge-specific MAE:\")\n",
        "        # for age, metrics in sorted(age_metrics.items()):\n",
        "        #     print(f\"Age {age}: {metrics['mae']:.3f} years, Within ±1 year: {metrics['within_1yr']:.1%} ({metrics['samples']} samples)\")\n",
        "        for age, m in sorted(age_metrics.items()):\n",
        "          print(f\"Age {age}: {m['mae']:.3f} years, Within ±1 year: {m['within_1yr']:.1%} ({m['samples']} samples)\")\n",
        "\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "Pl-2ctDaqOq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "# Visualization functions for evaluation results\n",
        "def visualize_binary_results(metrics):\n",
        "    \"\"\"Visualize binary classification results with clinical metrics\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(metrics['targets'], metrics['probabilities'])\n",
        "    roc_auc = metrics['roc_auc']\n",
        "\n",
        "    ax1.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "    ax1.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    ax1.set_xlim([0.0, 1.0])\n",
        "    ax1.set_ylim([0.0, 1.05])\n",
        "    ax1.set_xlabel('False Positive Rate')\n",
        "    ax1.set_ylabel('True Positive Rate')\n",
        "    ax1.set_title('Receiver Operating Characteristic')\n",
        "    ax1.legend(loc=\"lower right\")\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = metrics['confusion_matrix']\n",
        "    ax2.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    ax2.set_title('Confusion Matrix')\n",
        "    tick_marks = np.arange(2)\n",
        "    ax2.set_xticks(tick_marks)\n",
        "    ax2.set_yticks(tick_marks)\n",
        "    ax2.set_xticklabels(['Minor (<18)', 'Adult (≥18)'])\n",
        "    ax2.set_yticklabels(['Minor (<18)', 'Adult (≥18)'])\n",
        "    ax2.set_ylabel('True Label')\n",
        "    ax2.set_xlabel('Predicted Label')\n",
        "\n",
        "    # Add text annotations to confusion matrix\n",
        "    thresh = cm.max() / 2\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax2.text(j, i, format(cm[i, j], 'd'),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"binary_evaluation_results.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Bar chart for sensitivity and specificity with confidence intervals\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    metrics_names = ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1 Score']\n",
        "    metrics_values = [\n",
        "        metrics['accuracy'],\n",
        "        metrics['sensitivity'],\n",
        "        metrics['specificity'],\n",
        "        metrics['precision'],\n",
        "        metrics['f1']\n",
        "    ]\n",
        "\n",
        "    # Add confidence intervals for metrics that have them\n",
        "    yerr = np.zeros((2, 5))\n",
        "    yerr[0, 0] = metrics['accuracy'] - metrics['accuracy_ci'][0]  # lower error for accuracy\n",
        "    yerr[1, 0] = metrics['accuracy_ci'][1] - metrics['accuracy']  # upper error for accuracy\n",
        "    yerr[0, 1] = metrics['sensitivity'] - metrics['sensitivity_ci'][0]  # lower error for sensitivity\n",
        "    yerr[1, 1] = metrics['sensitivity_ci'][1] - metrics['sensitivity']  # upper error for sensitivity\n",
        "    yerr[0, 2] = metrics['specificity'] - metrics['specificity_ci'][0]  # lower error for specificity\n",
        "    yerr[1, 2] = metrics['specificity_ci'][1] - metrics['specificity']  # upper error for specificity\n",
        "\n",
        "    bars = plt.bar(metrics_names, metrics_values, yerr=yerr, capsize=10, alpha=0.7)\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                f'{height:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.ylim(0, 1.15)  # Set y-axis limit with space for error bars\n",
        "    plt.title('Binary Classification Metrics with 95% Confidence Intervals')\n",
        "    plt.ylabel('Score')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"binary_metrics_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_multiclass_results(metrics):\n",
        "    \"\"\"Visualize multiclass classification results\"\"\"\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = metrics['confusion_matrix']\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(n_classes)\n",
        "    plt.xticks(tick_marks, [str(class2age[i]) for i in range(n_classes)], rotation=45)\n",
        "    plt.yticks(tick_marks, [str(class2age[i]) for i in range(n_classes)])\n",
        "    plt.xlabel('Predicted Age')\n",
        "    plt.ylabel('True Age')\n",
        "\n",
        "    # Add text annotations\n",
        "    thresh = cm.max() / 2\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                    fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"multiclass_confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot per-class metrics\n",
        "    class_report = metrics['class_report']\n",
        "    ages = [class2age[i] for i in range(n_classes)]\n",
        "    precisions = [class_report[str(age)]['precision'] for age in ages]\n",
        "    recalls = [class_report[str(age)]['recall'] for age in ages]\n",
        "    f1_scores = [class_report[str(age)]['f1-score'] for age in ages]\n",
        "    supports = [class_report[str(age)]['support'] for age in ages]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = np.arange(len(ages))\n",
        "    width = 0.25\n",
        "\n",
        "    plt.bar(x - width, precisions, width, label='Precision', alpha=0.7)\n",
        "    plt.bar(x, recalls, width, label='Recall', alpha=0.7)\n",
        "    plt.bar(x + width, f1_scores, width, label='F1 Score', alpha=0.7)\n",
        "\n",
        "    plt.axvline(x=4-0.5, color='red', linestyle='--', alpha=0.5, label='Adult threshold (18 years)')\n",
        "\n",
        "    plt.xlabel('Age')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Performance Metrics by Age Class')\n",
        "    plt.xticks(x, ages)\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add sample size annotations\n",
        "    for i, support in enumerate(supports):\n",
        "        plt.text(i, 0.05, f'n={support}', ha='center', rotation=90, alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"multiclass_metrics_by_age.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def visualize_regression_results(metrics):\n",
        "    \"\"\"Visualize regression results\"\"\"\n",
        "    # predictions = metrics['predictions']\n",
        "    # targets = metrics['targets']\n",
        "    predictions = np.array(metrics.get('predictions', []))\n",
        "    targets = np.array(metrics.get('targets', []))\n",
        "\n",
        "    if len(predictions) == 0 or len(targets) == 0:\n",
        "      print(\"Regression metrics missing predictions. Skipping regression plots.\")\n",
        "      return\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Scatter plot: Predicted vs. True\n",
        "    ax1.scatter(targets, predictions, alpha=0.6, label=\"Predictions\")\n",
        "    ax1.plot([age_min, age_max], [age_min, age_max], 'r--', label=\"Ideal\")\n",
        "    ax1.set_xlabel(\"True Age\")\n",
        "    ax1.set_ylabel(\"Predicted Age\")\n",
        "    ax1.set_title(\"Predicted vs. True Age\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_xlim(age_min - 0.5, age_max + 0.5)\n",
        "    ax1.set_ylim(age_min - 0.5, age_max + 0.5)\n",
        "\n",
        "    # Residuals plot\n",
        "    residuals = predictions - targets\n",
        "    ax2.scatter(targets, residuals, alpha=0.6)\n",
        "    ax2.axhline(y=0, color='r', linestyle='--')\n",
        "    ax2.set_xlabel(\"True Age\")\n",
        "    ax2.set_ylabel(\"Residuals (Predicted - True)\")\n",
        "    ax2.set_title(\"Residuals\")\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_xlim(age_min - 0.5, age_max + 0.5)\n",
        "\n",
        "    # Age-specific performance\n",
        "    ages = list(sorted(metrics['age_metrics'].keys()))\n",
        "    mae_values = [metrics['age_metrics'][age]['mae'] for age in ages]\n",
        "    within_1yr = [metrics['age_metrics'][age]['within_1yr'] * 100 for age in ages]\n",
        "    sample_sizes = [metrics['age_metrics'][age]['samples'] for age in ages]\n",
        "\n",
        "    ax3.bar(ages, within_1yr, alpha=0.7)\n",
        "    ax3.set_xlabel(\"Age (years)\")\n",
        "    ax3.set_ylabel(\"% within ±1 year\")\n",
        "    ax3.set_title(\"Age-specific Performance\")\n",
        "    ax3.axvline(x=18, color='red', linestyle='--', alpha=0.7, label='Adult threshold')\n",
        "    ax3.legend()\n",
        "    ax3.set_ylim(0, 100)\n",
        "    ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add sample size annotations\n",
        "    for i, (age, count) in enumerate(zip(ages, sample_sizes)):\n",
        "        ax3.text(age, within_1yr[i] + 3, f\"n={count}\", ha='center', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"regression_evaluation_results.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Box plot of absolute errors by age\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    error_by_age = []\n",
        "    age_labels = []\n",
        "\n",
        "    for age in range(age_min, age_max + 1):\n",
        "        age_indices = [i for i, y in enumerate(targets) if int(y) == age]\n",
        "        if age_indices:\n",
        "            age_errors = np.abs(predictions[age_indices] - targets[age_indices])\n",
        "            error_by_age.append(age_errors)\n",
        "            age_labels.append(str(age))\n",
        "\n",
        "    plt.boxplot(error_by_age, labels=age_labels)\n",
        "    plt.axvline(x=4.5, color='red', linestyle='--', alpha=0.5, label='Adult threshold (18 years)')\n",
        "    plt.xlabel('Age')\n",
        "    plt.ylabel('Absolute Error (years)')\n",
        "    plt.title('Distribution of Prediction Errors by Age')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"regression_error_boxplot.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Oo9d5awQqOm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam --quiet"
      ],
      "metadata": {
        "id": "48FMNX0q5Dc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_vit_interpretability(model, val_df, task_type='binary'):\n",
        "    \"\"\"\n",
        "    Visualize ViT model interpretability using a gradient-based approach\n",
        "\n",
        "    Args:\n",
        "        model: The ViT model to visualize\n",
        "        val_df: Validation dataframe\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "    \"\"\"\n",
        "    # Get validation transform for ViT (224x224 input)\n",
        "    _, val_transform = get_transforms(augment=False, backbone='vit')\n",
        "\n",
        "    # Define age groups\n",
        "    age_groups = [\n",
        "        (14, 17, \"Minor (<18)\"),\n",
        "        (18, 21, \"Young Adult (18-21)\"),\n",
        "        (22, 24, \"Adult (22-24)\")\n",
        "    ]\n",
        "\n",
        "    # Configure figure for visualizations\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    for i, (min_age, max_age, group_label) in enumerate(age_groups):\n",
        "        # Find samples in this age range\n",
        "        group_samples = val_df[(val_df['age'] >= min_age) & (val_df['age'] <= max_age)]\n",
        "        if len(group_samples) == 0:\n",
        "            print(f\"No samples found for age group {min_age}-{max_age}\")\n",
        "            continue\n",
        "\n",
        "        # Select one random sample\n",
        "        sample = group_samples.sample(1).iloc[0]\n",
        "        true_age = sample['age']\n",
        "\n",
        "        # Load and preprocess image\n",
        "        img_pil = Image.open(sample['filepath']).convert('RGB')\n",
        "\n",
        "        # For visualization\n",
        "        img_np = np.array(img_pil.resize((224, 224))) / 255.0\n",
        "\n",
        "        # For model input\n",
        "        img_tensor = val_transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        # Original image\n",
        "        plt.subplot(3, 3, 3*i+1)\n",
        "        plt.imshow(img_pil)\n",
        "        plt.title(f\"Original - Age: {true_age}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Get prediction and generate feature attributions\n",
        "        model.eval()\n",
        "\n",
        "        # Use occlusion sensitivity as a simple alternative to attention maps\n",
        "        # We'll create a heatmap by occluding parts of the image and measuring impact on prediction\n",
        "        patch_size = 16  # Default patch size in ViT\n",
        "        num_patches = 224 // patch_size  # Should be 14 for 224x224 input\n",
        "\n",
        "        # Initialize heatmap\n",
        "        sensitivity_map = np.zeros((num_patches, num_patches))\n",
        "\n",
        "        # Get baseline prediction\n",
        "        with torch.no_grad():\n",
        "            if task_type == 'binary':\n",
        "                baseline_pred = model(img_tensor).item()\n",
        "                pred_label = \"Adult\" if baseline_pred >= 0.5 else \"Minor\"\n",
        "            elif task_type == 'multiclass':\n",
        "                output = model(img_tensor)\n",
        "                pred_class = torch.argmax(output, dim=1).item()\n",
        "                baseline_pred = pred_class\n",
        "                pred = class2age[pred_class]\n",
        "                pred_label = f\"{pred} years\"\n",
        "            else:  # regression\n",
        "                baseline_pred = model(img_tensor).item()\n",
        "                pred = denormalize_age(baseline_pred)\n",
        "                pred_label = f\"{pred:.1f} years\"\n",
        "\n",
        "        # Generate occlusion sensitivity map\n",
        "        for h in range(num_patches):\n",
        "            for w in range(num_patches):\n",
        "                # Create occluded image\n",
        "                occluded_tensor = img_tensor.clone()\n",
        "\n",
        "                # Apply occlusion patch\n",
        "                occluded_tensor[0, :, h*patch_size:(h+1)*patch_size, w*patch_size:(w+1)*patch_size] = 0\n",
        "\n",
        "                # Get prediction with occlusion\n",
        "                with torch.no_grad():\n",
        "                    if task_type == 'binary':\n",
        "                        occluded_pred = model(occluded_tensor).item()\n",
        "                    elif task_type == 'multiclass':\n",
        "                        output = model(occluded_tensor)\n",
        "                        occluded_pred = torch.argmax(output, dim=1).item()\n",
        "                    else:  # regression\n",
        "                        occluded_pred = model(occluded_tensor).item()\n",
        "\n",
        "                # Measure impact (change in prediction)\n",
        "                impact = abs(baseline_pred - occluded_pred)\n",
        "                sensitivity_map[h, w] = impact\n",
        "\n",
        "        # Normalize sensitivity map\n",
        "        sensitivity_map = (sensitivity_map - sensitivity_map.min()) / (sensitivity_map.max() - sensitivity_map.min() + 1e-8)\n",
        "\n",
        "        # Resize to image dimensions\n",
        "        import cv2\n",
        "        resized_map = cv2.resize(sensitivity_map, (224, 224))\n",
        "\n",
        "        # Display sensitivity map\n",
        "        plt.subplot(3, 3, 3*i+2)\n",
        "        plt.imshow(resized_map, cmap='jet')\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "        plt.title(f\"ViT Sensitivity Map - Pred: {pred_label}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Display overlay\n",
        "        plt.subplot(3, 3, 3*i+3)\n",
        "        plt.imshow(img_np)\n",
        "        plt.imshow(resized_map, alpha=0.5, cmap='jet')\n",
        "        plt.title(f\"ViT Sensitivity Overlay - Pred: {pred_label}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"vit_interpretability.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return \"ViT interpretability visualization completed\"\n",
        "\n",
        "\n",
        "def visualize_gradcam_simple(model, val_df, task_type='binary', backbone='efficientnet'):\n",
        "    \"\"\"\n",
        "    A simplified GradCAM visualization function that works with both EfficientNet and ViT.\n",
        "    Shows examples from each age group with original images only (no heatmaps).\n",
        "    \"\"\"\n",
        "    # Create validation transform specific to this backbone\n",
        "    _, val_transform = get_transforms(augment=False, backbone=backbone)\n",
        "\n",
        "    # Define age groups\n",
        "    age_groups = [\n",
        "        (14, 17, \"Minor (<18)\"),\n",
        "        (18, 21, \"Young Adult (18-21)\"),\n",
        "        (22, 24, \"Adult (22-24)\")\n",
        "    ]\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    for i, (min_age, max_age, group_label) in enumerate(age_groups):\n",
        "        # Find samples in this age range\n",
        "        group_samples = val_df[(val_df['age'] >= min_age) & (val_df['age'] <= max_age)]\n",
        "        if len(group_samples) == 0:\n",
        "            print(f\"No samples found for age group {min_age}-{max_age}\")\n",
        "            continue\n",
        "\n",
        "        # Select one random sample\n",
        "        sample = group_samples.sample(1).iloc[0]\n",
        "        true_age = sample['age']\n",
        "\n",
        "        # Create a subplot for this sample\n",
        "        plt.subplot(3, 1, i+1)\n",
        "\n",
        "        # Load and display original image\n",
        "        img_pil = Image.open(sample['filepath']).convert('RGB')\n",
        "        plt.imshow(img_pil)\n",
        "\n",
        "        # Prepare image for model\n",
        "        img_tensor = val_transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get prediction\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            if task_type == 'binary':\n",
        "                pred = model(img_tensor).item()\n",
        "                pred_label = \"Adult\" if pred >= 0.5 else \"Minor\"\n",
        "            elif task_type == 'multiclass':\n",
        "                output = model(img_tensor)\n",
        "                pred_class = torch.argmax(output, dim=1).item()\n",
        "                pred = class2age[pred_class]\n",
        "                pred_label = f\"{pred} years\"\n",
        "            else:  # regression\n",
        "                pred_norm = model(img_tensor).item()\n",
        "                pred = denormalize_age(pred_norm)\n",
        "                pred_label = f\"{pred:.1f} years\"\n",
        "\n",
        "        plt.title(f\"{group_label} - True Age: {true_age}, Predicted: {pred_label}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"model_prediction_{task_type}_{backbone}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return \"Visualization completed\"\n",
        "\n",
        "\n",
        "def visualize_model_interpretability(model, val_df, task_type='binary', backbone='efficientnet'):\n",
        "    \"\"\"\n",
        "    Visualize model interpretability:\n",
        "    - GradCAM for EfficientNet\n",
        "    - Occlusion sensitivity for ViT\n",
        "\n",
        "    Args:\n",
        "        model: The model to visualize\n",
        "        val_df: Validation dataframe\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "        backbone: 'efficientnet' or 'vit'\n",
        "    \"\"\"\n",
        "    if backbone == 'efficientnet':\n",
        "        # For EfficientNet: Use GradCAM\n",
        "        try:\n",
        "            from pytorch_grad_cam import GradCAM\n",
        "            from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "            # Create validation transform specific to this backbone\n",
        "            _, val_transform = get_transforms(augment=False, backbone=backbone)\n",
        "\n",
        "            target_layers = [model.backbone.features[-1]]\n",
        "\n",
        "            # Define age groups\n",
        "            age_groups = [\n",
        "                (14, 17, \"Minor (<18)\"),\n",
        "                (18, 21, \"Young Adult (18-21)\"),\n",
        "                (22, 24, \"Adult (22-24)\")\n",
        "            ]\n",
        "\n",
        "            # Configure figure for original images and GradCAM visualizations\n",
        "            plt.figure(figsize=(12, 15))\n",
        "\n",
        "            for i, (min_age, max_age, group_label) in enumerate(age_groups):\n",
        "                # Find samples in this age range\n",
        "                group_samples = val_df[(val_df['age'] >= min_age) & (val_df['age'] <= max_age)]\n",
        "                if len(group_samples) == 0:\n",
        "                    print(f\"No samples found for age group {min_age}-{max_age}\")\n",
        "                    continue\n",
        "\n",
        "                # Select one random sample\n",
        "                sample = group_samples.sample(1).iloc[0]\n",
        "                true_age = sample['age']\n",
        "\n",
        "                # Load and preprocess image\n",
        "                img_pil = Image.open(sample['filepath']).convert('RGB')\n",
        "                img_tensor = val_transform(img_pil).unsqueeze(0).to(device)\n",
        "\n",
        "                # Get prediction\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if task_type == 'binary':\n",
        "                        pred = model(img_tensor).item()\n",
        "                        pred_label = \"Adult\" if pred >= 0.5 else \"Minor\"\n",
        "                    elif task_type == 'multiclass':\n",
        "                        output = model(img_tensor)\n",
        "                        pred_class = torch.argmax(output, dim=1).item()\n",
        "                        pred = class2age[pred_class]\n",
        "                        pred_label = f\"{pred} years\"\n",
        "                    else:  # regression\n",
        "                        pred_norm = model(img_tensor).item()\n",
        "                        pred = denormalize_age(pred_norm)\n",
        "                        pred_label = f\"{pred:.1f} years\"\n",
        "\n",
        "                # Original image\n",
        "                plt.subplot(3, 2, 2*i+1)\n",
        "                plt.imshow(img_pil)\n",
        "                plt.title(f\"Original - Age: {true_age}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                try:\n",
        "                    # Custom target for GradCAM - simplified for all model types\n",
        "                    class CustomTarget:\n",
        "                        def __call__(self, model_output):\n",
        "                            # Handle scalar outputs (like binary)\n",
        "                            if isinstance(model_output, torch.Tensor):\n",
        "                                if len(model_output.shape) == 0 or model_output.shape == torch.Size([1]):\n",
        "                                    return model_output\n",
        "                                # Handle vector outputs (like multiclass)\n",
        "                                if len(model_output.shape) > 0:\n",
        "                                    return torch.max(model_output)\n",
        "                            return model_output\n",
        "\n",
        "                    # Apply GradCAM\n",
        "                    cam = GradCAM(model=model, target_layers=target_layers)\n",
        "                    grayscale_cam = cam(input_tensor=img_tensor, targets=[CustomTarget()])[0]\n",
        "\n",
        "                    # Normalize the image for visualization\n",
        "                    input_np = img_tensor[0].cpu().numpy().transpose(1, 2, 0)\n",
        "                    input_np = (input_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])).clip(0, 1)\n",
        "\n",
        "                    # Create GradCAM visualization\n",
        "                    visualization = show_cam_on_image(input_np, grayscale_cam, use_rgb=True)\n",
        "\n",
        "                    # Display GradCAM\n",
        "                    plt.subplot(3, 2, 2*i+2)\n",
        "                    plt.imshow(visualization)\n",
        "                    plt.title(f\"GradCAM - Pred: {pred_label}\")\n",
        "                    plt.axis('off')\n",
        "                except Exception as e:\n",
        "                    print(f\"GradCAM error: {e}\")\n",
        "                    plt.subplot(3, 2, 2*i+2)\n",
        "                    plt.text(0.5, 0.5, f\"GradCAM error: {str(e)}\",\n",
        "                            ha='center', va='center', transform=plt.gca().transAxes)\n",
        "                    plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"gradcam_{task_type}_{backbone}.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Could not import pytorch_grad_cam. Falling back to simple visualization.\")\n",
        "            visualize_gradcam_simple(model, val_df, task_type, backbone)\n",
        "\n",
        "    else:  # For ViT: Use occlusion sensitivity visualization\n",
        "        visualize_vit_interpretability(model, val_df, task_type)\n",
        "\n",
        "    return \"Visualization completed\""
      ],
      "metadata": {
        "id": "EgjPyrN1owTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fixed version of analyze_sex_differences function to handle all task types\n",
        "def analyze_sex_differences(df_fixed, task_type='binary'):\n",
        "    \"\"\"\n",
        "    Analyze and compare model performance by sex\n",
        "\n",
        "    Args:\n",
        "        df_fixed: DataFrame with the dataset\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with performance metrics by sex\n",
        "    \"\"\"\n",
        "    # Prepare data by sex\n",
        "    train_loader_m, val_loader_m, train_df_m, val_df_m = prepare_data(\n",
        "        df_fixed, task_type=task_type, augment=True, sex_filter='M', backbone='efficientnet')\n",
        "\n",
        "    train_loader_f, val_loader_f, train_df_f, val_df_f = prepare_data(\n",
        "        df_fixed, task_type=task_type, augment=True, sex_filter='F', backbone='efficientnet')\n",
        "\n",
        "    # Train male-specific model\n",
        "    print(\"\\n===== Training Male-Specific Model =====\")\n",
        "    model_m = DentalAgeModel(task_type=task_type, backbone='efficientnet', use_attention=True).to(device)\n",
        "    model_m, train_losses_m, val_losses_m, _ = train_model(model_m, train_loader_m, val_loader_m, task_type=task_type, num_epochs=15)\n",
        "\n",
        "    # Train female-specific model\n",
        "    print(\"\\n===== Training Female-Specific Model =====\")\n",
        "    model_f = DentalAgeModel(task_type=task_type, backbone='efficientnet', use_attention=True).to(device)\n",
        "    model_f, train_losses_f, val_losses_f, _ = train_model(model_f, train_loader_f, val_loader_f, task_type=task_type, num_epochs=15)\n",
        "\n",
        "    # Evaluate models\n",
        "    print(\"\\n===== Evaluating Male-Specific Model =====\")\n",
        "    metrics_m = evaluate_model(model_m, val_loader_m, task_type=task_type)\n",
        "\n",
        "    print(\"\\n===== Evaluating Female-Specific Model =====\")\n",
        "    metrics_f = evaluate_model(model_f, val_loader_f, task_type=task_type)\n",
        "\n",
        "    # Visualize model predictions\n",
        "    print(\"\\n===== Visualizing Male Model Predictions =====\")\n",
        "    visualize_gradcam_simple(model_m, val_df_m, task_type=task_type, backbone='efficientnet')\n",
        "\n",
        "    print(\"\\n===== Visualizing Female Model Predictions =====\")\n",
        "    visualize_gradcam_simple(model_f, val_df_f, task_type=task_type, backbone='efficientnet')\n",
        "\n",
        "    # Compare results\n",
        "    print(\"\\n===== Sex-Specific Performance Comparison =====\")\n",
        "\n",
        "    # Set up comparison metrics based on task type\n",
        "    if task_type == 'binary':\n",
        "        # Create metrics dict mapping display names to actual keys in the metrics dict\n",
        "        metrics_mapping = {\n",
        "            'Accuracy': 'accuracy',\n",
        "            'Sensitivity': 'sensitivity',\n",
        "            'Specificity': 'specificity',\n",
        "            'F1 Score': 'f1'  # The key is 'f1', not 'f1 score'\n",
        "        }\n",
        "\n",
        "        metrics = {\n",
        "            'Male': {\n",
        "                'accuracy': metrics_m.get('accuracy', 0),\n",
        "                'sensitivity': metrics_m.get('sensitivity', 0),\n",
        "                'specificity': metrics_m.get('specificity', 0),\n",
        "                'f1': metrics_m.get('f1', 0)\n",
        "            },\n",
        "            'Female': {\n",
        "                'accuracy': metrics_f.get('accuracy', 0),\n",
        "                'sensitivity': metrics_f.get('sensitivity', 0),\n",
        "                'specificity': metrics_f.get('specificity', 0),\n",
        "                'f1': metrics_f.get('f1', 0)\n",
        "            }\n",
        "        }\n",
        "        metrics_names = ['Accuracy', 'Sensitivity', 'Specificity', 'F1 Score']\n",
        "        male_values = [metrics['Male'][metrics_mapping[k]] for k in metrics_names]\n",
        "        female_values = [metrics['Female'][metrics_mapping[k]] for k in metrics_names]\n",
        "\n",
        "    elif task_type == 'multiclass':\n",
        "        metrics_mapping = {\n",
        "            'Accuracy': 'accuracy',\n",
        "            'MAE': 'mae',\n",
        "            'RMSE': 'rmse'\n",
        "        }\n",
        "\n",
        "        metrics = {\n",
        "            'Male': {\n",
        "                'accuracy': metrics_m.get('accuracy', 0),\n",
        "                'mae': metrics_m.get('mae', 0),\n",
        "                'rmse': metrics_m.get('rmse', 0)\n",
        "            },\n",
        "            'Female': {\n",
        "                'accuracy': metrics_f.get('accuracy', 0),\n",
        "                'mae': metrics_f.get('mae', 0),\n",
        "                'rmse': metrics_f.get('rmse', 0)\n",
        "            }\n",
        "        }\n",
        "        metrics_names = ['Accuracy', 'MAE', 'RMSE']\n",
        "        male_values = [metrics['Male'][metrics_mapping[k]] for k in metrics_names]\n",
        "        female_values = [metrics['Female'][metrics_mapping[k]] for k in metrics_names]\n",
        "\n",
        "    else:  # regression\n",
        "        metrics_mapping = {\n",
        "            'MAE': 'mae',\n",
        "            'RMSE': 'rmse',\n",
        "            'R²': 'r2',\n",
        "            'Binary Accuracy': 'binary_acc'\n",
        "        }\n",
        "\n",
        "        metrics = {\n",
        "            'Male': {\n",
        "                'mae': metrics_m.get('mae', 0),\n",
        "                'rmse': metrics_m.get('rmse', 0),\n",
        "                'r2': metrics_m.get('r2', 0),\n",
        "                'binary_acc': metrics_m.get('binary_acc', 0)\n",
        "            },\n",
        "            'Female': {\n",
        "                'mae': metrics_f.get('mae', 0),\n",
        "                'rmse': metrics_f.get('rmse', 0),\n",
        "                'r2': metrics_f.get('r2', 0),\n",
        "                'binary_acc': metrics_f.get('binary_acc', 0)\n",
        "            }\n",
        "        }\n",
        "        metrics_names = ['MAE', 'RMSE', 'R²', 'Binary Accuracy']\n",
        "        male_values = [metrics['Male'][metrics_mapping[k]] for k in metrics_names]\n",
        "        female_values = [metrics['Female'][metrics_mapping[k]] for k in metrics_names]\n",
        "\n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    x = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, male_values, width, label='Male', color='royalblue', alpha=0.7)\n",
        "    plt.bar(x + width/2, female_values, width, label='Female', color='darkorange', alpha=0.7)\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(f'{task_type.capitalize()} Performance: Male vs Female Models')\n",
        "    plt.xticks(x, metrics_names)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for i, v in enumerate(male_values):\n",
        "        plt.text(i - width/2, v + 0.02, f'{v:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    for i, v in enumerate(female_values):\n",
        "        plt.text(i + width/2, v + 0.02, f'{v:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # Adjust y-axis limits based on task type\n",
        "    if task_type == 'regression':\n",
        "        plt.ylim(0, max([v for v in male_values + female_values if v is not None]) * 1.2)\n",
        "    else:\n",
        "        plt.ylim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"sex_specific_comparison_{task_type}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "Kd0DgtY9owHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compare_backbones(df_fixed, task_type='binary'):\n",
        "    \"\"\"Compare ViT and EfficientNet performance\"\"\"\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Prepare data for each backbone\n",
        "    train_loader_eff, val_loader_eff, train_df_eff, val_df_eff = prepare_data(\n",
        "        df_fixed, task_type=task_type, augment=True, backbone='efficientnet')\n",
        "    train_loader_vit, val_loader_vit, train_df_vit, val_df_vit = prepare_data(\n",
        "        df_fixed, task_type=task_type, augment=True, backbone='vit')\n",
        "\n",
        "    # Train EfficientNet model\n",
        "    print(\"\\n===== Training EfficientNet Model =====\")\n",
        "    model_eff = DentalAgeModel(task_type=task_type, backbone='efficientnet', use_attention=True).to(device)\n",
        "    model_eff, train_losses_eff, val_losses_eff, _ = train_model(\n",
        "        model_eff, train_loader_eff, val_loader_eff, task_type=task_type, num_epochs=25)\n",
        "\n",
        "    # Train ViT model\n",
        "    print(\"\\n===== Training Vision Transformer Model =====\")\n",
        "    model_vit = DentalAgeModel(task_type=task_type, backbone='vit', use_attention=False).to(device)\n",
        "    model_vit, train_losses_vit, val_losses_vit, _ = train_model(\n",
        "        model_vit, train_loader_vit, val_loader_vit, task_type=task_type, num_epochs=25)\n",
        "\n",
        "    # Evaluate models\n",
        "    print(\"\\n===== Evaluating EfficientNet Model =====\")\n",
        "    metrics_eff = evaluate_model(model_eff, val_loader_eff, task_type=task_type)\n",
        "    print(\"EfficientNet metrics:\", metrics_eff)\n",
        "    print(\"\\n===== Evaluating Vision Transformer Model =====\")\n",
        "    metrics_vit = evaluate_model(model_vit, val_loader_vit, task_type=task_type)\n",
        "    print(\"ViT metrics:\", metrics_vit)\n",
        "\n",
        "    # Visualization\n",
        "    print(\"\\n===== Visualizing EfficientNet Model Interpretability (GradCAM) =====\")\n",
        "    visualize_model_interpretability(model_eff, val_df_eff, task_type=task_type, backbone='efficientnet')\n",
        "    print(\"\\n===== Visualizing Vision Transformer Model Interpretability (Attention Maps) =====\")\n",
        "    visualize_model_interpretability(model_vit, val_df_vit, task_type=task_type, backbone='vit')\n",
        "\n",
        "    # Compare metrics and assign plotting variables\n",
        "    if task_type == 'binary':\n",
        "        metrics_names = ['Accuracy', 'Sensitivity', 'Specificity', 'F1 Score', 'ROC AUC']\n",
        "        eff_values = [\n",
        "            metrics_eff.get('accuracy', 0),\n",
        "            metrics_eff.get('sensitivity', 0),\n",
        "            metrics_eff.get('specificity', 0),\n",
        "            metrics_eff.get('f1', 0),\n",
        "            metrics_eff.get('roc_auc', 0)\n",
        "        ]\n",
        "        vit_values = [\n",
        "            metrics_vit.get('accuracy', 0),\n",
        "            metrics_vit.get('sensitivity', 0),\n",
        "            metrics_vit.get('specificity', 0),\n",
        "            metrics_vit.get('f1', 0),\n",
        "            metrics_vit.get('roc_auc', 0)\n",
        "        ]\n",
        "    elif task_type == 'multiclass':\n",
        "        metrics_names = ['Accuracy', 'MAE', 'RMSE']\n",
        "        eff_values = [\n",
        "            metrics_eff.get('accuracy', 0),\n",
        "            metrics_eff.get('mae', 0),\n",
        "            metrics_eff.get('rmse', 0)\n",
        "        ]\n",
        "        vit_values = [\n",
        "            metrics_vit.get('accuracy', 0),\n",
        "            metrics_vit.get('mae', 0),\n",
        "            metrics_vit.get('rmse', 0)\n",
        "        ]\n",
        "    elif task_type == 'regression':\n",
        "        metrics_names = ['MAE', 'RMSE', 'R²', 'Binary Accuracy']\n",
        "        eff_values = [\n",
        "            metrics_eff.get('mae', 0),\n",
        "            metrics_eff.get('rmse', 0),\n",
        "            metrics_eff.get('r2', 0),\n",
        "            metrics_eff.get('binary_acc', 0)\n",
        "        ]\n",
        "        vit_values = [\n",
        "            metrics_vit.get('mae', 0),\n",
        "            metrics_vit.get('rmse', 0),\n",
        "            metrics_vit.get('r2', 0),\n",
        "            metrics_vit.get('binary_acc', 0)\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown task_type: {task_type}\")\n",
        "\n",
        "    # Plot comparison\n",
        "    plt.figure(figsize=(12, 6) if task_type != 'regression' else (10, 6))\n",
        "    x = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "    plt.bar(x - width/2, eff_values, width, label='EfficientNet', color='royalblue', alpha=0.7)\n",
        "    plt.bar(x + width/2, vit_values, width, label='Vision Transformer', color='darkorange', alpha=0.7)\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(f'{task_type.capitalize()} Task: EfficientNet vs Vision Transformer')\n",
        "    plt.xticks(x, metrics_names)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    for i, v in enumerate(eff_values):\n",
        "        if v is not None:\n",
        "            plt.text(i - width/2, v + 0.02, f'{v:.2f}', ha='center', va='bottom')\n",
        "    for i, v in enumerate(vit_values):\n",
        "        if v is not None:\n",
        "            plt.text(i + width/2, v + 0.02, f'{v:.2f}', ha='center', va='bottom')\n",
        "\n",
        "    # Defensive: filter None for ylim\n",
        "    all_scores = [v for v in eff_values + vit_values if v is not None]\n",
        "    if task_type == 'regression':\n",
        "        if all_scores:\n",
        "            plt.ylim(0, max(all_scores) * 1.2)\n",
        "    else:\n",
        "        plt.ylim(0, 1.1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"backbone_comparison_{task_type}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Compare training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses_eff, label='EfficientNet Train')\n",
        "    plt.plot(val_losses_eff, label='EfficientNet Val')\n",
        "    plt.plot(train_losses_vit, label='ViT Train', linestyle='--')\n",
        "    plt.plot(val_losses_vit, label='ViT Val', linestyle='--')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"training_curves_comparison_{task_type}.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'EfficientNet': dict(zip(metrics_names, eff_values)),\n",
        "        'ViT': dict(zip(metrics_names, vit_values))\n",
        "    }"
      ],
      "metadata": {
        "id": "K4x3jfxpov9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Streamlined run_dental_age_analysis function\n",
        "def run_dental_age_analysis(task_type='binary', augment=True, compare_models=True, sex_analysis=True, external_data_path=None):\n",
        "    \"\"\"\n",
        "    Run complete dental age analysis pipeline.\n",
        "\n",
        "    Args:\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "        augment: Whether to use data augmentation\n",
        "        compare_models: Whether to compare ViT and EfficientNet\n",
        "        sex_analysis: Whether to analyze sex differences\n",
        "        external_data_path: Path to external validation dataset (optional)\n",
        "    \"\"\"\n",
        "    print(f\"Running {task_type.upper()} dental age analysis pipeline with{'out' if not augment else ''} augmentation\")\n",
        "\n",
        "    # Prepare data\n",
        "    train_loader, val_loader, train_df, val_df = prepare_data(\n",
        "        df_fixed, task_type=task_type, augment=augment, backbone='efficientnet')\n",
        "\n",
        "    # Train model (default to EfficientNet)\n",
        "    print(f\"\\n===== Training {task_type.capitalize()} Model =====\")\n",
        "    model = DentalAgeModel(task_type=task_type, backbone='efficientnet', use_attention=True).to(device)\n",
        "    model, train_losses, val_losses, perf_history = train_model(model, train_loader, val_loader, task_type=task_type, num_epochs=25)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(f\"\\n===== Evaluating {task_type.capitalize()} Model =====\")\n",
        "    metrics = evaluate_model(model, val_loader, task_type=task_type)\n",
        "\n",
        "    # Visualize model interpretability (GradCAM for EfficientNet)\n",
        "    print(f\"\\n===== Visualizing {task_type.capitalize()} Model Interpretability =====\")\n",
        "    visualize_model_interpretability(model, val_df, task_type=task_type, backbone='efficientnet')\n",
        "\n",
        "    # Optional: Compare backbones\n",
        "    if compare_models:\n",
        "        print(\"\\n===== Comparing ViT and EfficientNet =====\")\n",
        "        # Use the dedicated function for backbone comparison\n",
        "        compare_metrics = compare_backbones(df_fixed, task_type=task_type)\n",
        "\n",
        "    # Optional: Analyze sex differences\n",
        "    if sex_analysis:\n",
        "        print(\"\\n===== Analyzing Sex Differences =====\")\n",
        "        sex_metrics = analyze_sex_differences(df_fixed, task_type=task_type)\n",
        "\n",
        "    print(\"\\n===== Analysis Complete =====\")\n",
        "    return model, metrics"
      ],
      "metadata": {
        "id": "4A01JsmJov2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def save_model_with_metadata(model, train_df, val_df, task_type='binary', backbone='efficientnet',\n",
        "                           augment=False, sex_filter=None, metrics=None):\n",
        "    \"\"\"\n",
        "    Save a trained model along with its metadata for future retrieval and evaluation.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model\n",
        "        train_df: Training dataframe\n",
        "        val_df: Validation dataframe\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "        backbone: 'efficientnet' or 'vit'\n",
        "        augment: Whether data augmentation was used\n",
        "        sex_filter: None, 'M', or 'F'\n",
        "        metrics: Evaluation metrics dictionary\n",
        "    \"\"\"\n",
        "    # Create a unique model identifier\n",
        "    sex_str = f\"_{sex_filter}\" if sex_filter else \"\"\n",
        "    aug_str = \"_aug\" if augment else \"\"\n",
        "    model_name = f\"{task_type}_{backbone}{sex_str}{aug_str}\"\n",
        "\n",
        "    # Define base directory in Google Drive\n",
        "    base_dir = \"/content/drive/MyDrive/DentalAgeClassification\"\n",
        "    model_dir = f\"{base_dir}/{model_name}\"\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Save the model weights\n",
        "    model_path = f\"{model_dir}/model.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model weights saved to {model_path}\")\n",
        "\n",
        "    # Save the dataframes\n",
        "    train_df_path = f\"{model_dir}/train_df.pkl\"\n",
        "    val_df_path = f\"{model_dir}/val_df.pkl\"\n",
        "    pd.to_pickle(train_df, train_df_path)\n",
        "    pd.to_pickle(val_df, val_df_path)\n",
        "    print(f\"Training data saved to {train_df_path}\")\n",
        "    print(f\"Validation data saved to {val_df_path}\")\n",
        "\n",
        "    # Process metrics to make them JSON serializable\n",
        "    if metrics:\n",
        "        serializable_metrics = {}\n",
        "        for key, value in metrics.items():\n",
        "            # Skip large arrays we don't need to save\n",
        "            if key in ['predictions', 'targets', 'probabilities', 'confusion_matrix']:\n",
        "                continue\n",
        "\n",
        "            # Convert numpy values to native Python types\n",
        "            if isinstance(value, np.ndarray):\n",
        "                serializable_metrics[key] = value.tolist()\n",
        "            elif isinstance(value, np.generic):  # For numpy scalars like np.float64\n",
        "                serializable_metrics[key] = value.item()\n",
        "            elif isinstance(value, dict):\n",
        "                # For nested dictionaries like age_metrics\n",
        "                serializable_subdict = {}\n",
        "                for subkey, subvalue in value.items():\n",
        "                    if isinstance(subvalue, dict):\n",
        "                        serializable_subsubdict = {}\n",
        "                        for subsubkey, subsubvalue in subvalue.items():\n",
        "                            if isinstance(subsubvalue, np.ndarray) or isinstance(subsubvalue, np.generic):\n",
        "                                serializable_subsubdict[subsubkey] = subsubvalue.item()\n",
        "                            else:\n",
        "                                serializable_subsubdict[subsubkey] = subsubvalue\n",
        "                        serializable_subdict[subkey] = serializable_subsubdict\n",
        "                    elif isinstance(subvalue, np.ndarray) or isinstance(subvalue, np.generic):\n",
        "                        serializable_subdict[subkey] = subvalue.item()\n",
        "                    else:\n",
        "                        serializable_subdict[subkey] = subvalue\n",
        "                serializable_metrics[key] = serializable_subdict\n",
        "            else:\n",
        "                serializable_metrics[key] = value\n",
        "    else:\n",
        "        serializable_metrics = {}\n",
        "\n",
        "    # Save configuration and metrics\n",
        "    config = {\n",
        "        'task_type': task_type,\n",
        "        'backbone': backbone,\n",
        "        'augmentation': augment,\n",
        "        'sex_filter': sex_filter,\n",
        "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'metrics': serializable_metrics,\n",
        "        'model_name': model_name\n",
        "    }\n",
        "\n",
        "    config_path = f\"{model_dir}/config.json\"\n",
        "    with open(config_path, 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "    print(f\"Model configuration saved to {config_path}\")\n",
        "\n",
        "    return model_name\n",
        "\n",
        "def load_model_with_metadata(model_name=None, task_type=None, backbone=None,\n",
        "                           sex_filter=None, augment=None):\n",
        "    \"\"\"\n",
        "    Load a saved model and its associated metadata.\n",
        "\n",
        "    Args:\n",
        "        model_name: Full model name for direct loading\n",
        "        task_type: Filter by task_type\n",
        "        backbone: Filter by backbone\n",
        "        sex_filter: Filter by sex\n",
        "        augment: Filter by augmentation\n",
        "\n",
        "    Returns:\n",
        "        model: Loaded model\n",
        "        config: Model configuration\n",
        "        train_df: Training dataframe\n",
        "        val_df: Validation dataframe\n",
        "    \"\"\"\n",
        "    base_dir = \"/content/drive/MyDrive/DentalAgeClassification\"\n",
        "\n",
        "    # If model_name is provided, load directly\n",
        "    if model_name:\n",
        "        model_dir = f\"{base_dir}/{model_name}\"\n",
        "    else:\n",
        "        # Find matching models based on filters\n",
        "        possible_models = []\n",
        "        for dirname in os.listdir(base_dir):\n",
        "            if not os.path.isdir(os.path.join(base_dir, dirname)):\n",
        "                continue\n",
        "\n",
        "            config_path = os.path.join(base_dir, dirname, \"config.json\")\n",
        "            if not os.path.exists(config_path):\n",
        "                continue\n",
        "\n",
        "            with open(config_path, 'r') as f:\n",
        "                config = json.load(f)\n",
        "\n",
        "            match = True\n",
        "            if task_type and config['task_type'] != task_type:\n",
        "                match = False\n",
        "            if backbone and config['backbone'] != backbone:\n",
        "                match = False\n",
        "            if sex_filter is not None and config['sex_filter'] != sex_filter:\n",
        "                match = False\n",
        "            if augment is not None and config['augmentation'] != augment:\n",
        "                match = False\n",
        "\n",
        "            if match:\n",
        "                possible_models.append((dirname, config['creation_date']))\n",
        "\n",
        "        if not possible_models:\n",
        "            print(\"No matching models found.\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        # Sort by creation date (newest first)\n",
        "        possible_models.sort(key=lambda x: x[1], reverse=True)\n",
        "        model_name = possible_models[0][0]\n",
        "        model_dir = f\"{base_dir}/{model_name}\"\n",
        "        print(f\"Loading most recent matching model: {model_name}\")\n",
        "\n",
        "    # Load configuration\n",
        "    config_path = f\"{model_dir}/config.json\"\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    # Load model\n",
        "    model_path = f\"{model_dir}/model.pth\"\n",
        "    model = DentalAgeModel(\n",
        "        task_type=config['task_type'],\n",
        "        backbone=config['backbone'],\n",
        "        use_attention=config['backbone'] == 'efficientnet'\n",
        "    ).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Load dataframes\n",
        "    train_df = pd.read_pickle(f\"{model_dir}/train_df.pkl\")\n",
        "    val_df = pd.read_pickle(f\"{model_dir}/val_df.pkl\")\n",
        "\n",
        "    print(f\"Model {model_name} loaded successfully\")\n",
        "    print(f\"Task type: {config['task_type']}, Backbone: {config['backbone']}\")\n",
        "    print(f\"Sex filter: {config['sex_filter']}, Augmentation: {config['augmentation']}\")\n",
        "    print(f\"Created on: {config['creation_date']}\")\n",
        "\n",
        "    return model, config, train_df, val_df\n",
        "\n",
        "def list_available_models():\n",
        "    \"\"\"List all available saved models with their configurations\"\"\"\n",
        "    base_dir = \"/content/drive/MyDrive/DentalAgeClassification\"\n",
        "    if not os.path.exists(base_dir):\n",
        "        print(f\"No models directory found at {base_dir}\")\n",
        "        return\n",
        "\n",
        "    models = []\n",
        "    for dirname in os.listdir(base_dir):\n",
        "        config_path = os.path.join(base_dir, dirname, \"config.json\")\n",
        "        if os.path.exists(config_path):\n",
        "            with open(config_path, 'r') as f:\n",
        "                config = json.load(f)\n",
        "            models.append(config)\n",
        "\n",
        "    if not models:\n",
        "        print(\"No models found.\")\n",
        "        return\n",
        "\n",
        "    # Sort by creation date\n",
        "    models.sort(key=lambda x: x.get('creation_date', ''), reverse=True)\n",
        "\n",
        "    # Display in table format\n",
        "    print(\"\\nAvailable Dental Age Models:\")\n",
        "    print(\"{:<25} {:<15} {:<15} {:<10} {:<10} {:<20}\".format(\n",
        "        \"Model Name\", \"Task Type\", \"Backbone\", \"Sex\", \"Augment\", \"Created\"))\n",
        "    print(\"-\" * 95)\n",
        "\n",
        "    for model in models:\n",
        "        print(\"{:<25} {:<15} {:<15} {:<10} {:<10} {:<20}\".format(\n",
        "            model['model_name'],\n",
        "            model['task_type'],\n",
        "            model['backbone'],\n",
        "            model.get('sex_filter', 'All') or 'All',\n",
        "            str(model['augmentation']),\n",
        "            model.get('creation_date', 'Unknown')\n",
        "        ))\n",
        "\n",
        "    return models\n",
        "\n",
        "def evaluate_saved_model(model_name=None, task_type=None, backbone=None, sex_filter=None,\n",
        "                        augment=None, external_df=None):\n",
        "    \"\"\"\n",
        "    Evaluate a saved model on validation or external data\n",
        "\n",
        "    Args:\n",
        "        model_name: Full model name for direct loading\n",
        "        task_type: Filter by task_type\n",
        "        backbone: Filter by backbone\n",
        "        sex_filter: Filter by sex\n",
        "        augment: Filter by augmentation\n",
        "        external_df: Optional external dataset for evaluation\n",
        "    \"\"\"\n",
        "    # Load the model and its metadata\n",
        "    model, config, train_df, val_df = load_model_with_metadata(\n",
        "        model_name, task_type, backbone, sex_filter, augment\n",
        "    )\n",
        "\n",
        "    if model is None:\n",
        "        return None\n",
        "\n",
        "    # Use the proper transform for the backbone\n",
        "    _, val_transform = get_transforms(augment=False, backbone=config['backbone'])\n",
        "\n",
        "    # Create dataset and dataloader for validation\n",
        "    if external_df is not None:\n",
        "        print(f\"Evaluating on external dataset with {len(external_df)} samples\")\n",
        "        dataset = DentalAgeDataset(external_df, val_transform, config['task_type'])\n",
        "    else:\n",
        "        print(f\"Evaluating on original validation set with {len(val_df)} samples\")\n",
        "        dataset = DentalAgeDataset(val_df, val_transform, config['task_type'])\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Evaluate the model\n",
        "    metrics = evaluate_model(model, dataloader, config['task_type'])\n",
        "\n",
        "    # Save updated metrics if using original validation set\n",
        "    if external_df is None:\n",
        "        config['metrics'] = metrics\n",
        "        config_path = f\"/content/drive/MyDrive/DentalAgeClassification/{config['model_name']}/config.json\"\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(config, f, indent=4)\n",
        "        print(f\"Updated metrics saved to {config_path}\")\n",
        "\n",
        "    # Visualize results\n",
        "    if config['task_type'] == 'binary':\n",
        "        visualize_binary_results(metrics)\n",
        "    elif config['task_type'] == 'multiclass':\n",
        "        visualize_multiclass_results(metrics)\n",
        "    else:  # regression\n",
        "        visualize_regression_results(metrics)\n",
        "\n",
        "    # Visualize interpretability for a few samples\n",
        "    sample_df = dataset.df.groupby('age', group_keys=False).apply(\n",
        "        lambda x: x.sample(min(1, len(x)))\n",
        "    ).sample(min(3, len(dataset.df['age'].unique())))\n",
        "\n",
        "    visualize_model_interpretability(model, sample_df, config['task_type'], config['backbone'])\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "4aeRrdQ6QJ7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_save_dental_model(task_type='binary', backbone='efficientnet', augment=True,\n",
        "                              sex_filter=None, num_epochs=25):\n",
        "    \"\"\"\n",
        "    Train a dental age model and save it with all metadata\n",
        "\n",
        "    Args:\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "        backbone: 'efficientnet' or 'vit'\n",
        "        augment: Whether to use data augmentation\n",
        "        sex_filter: None, 'M', or 'F' for gender-specific models\n",
        "        num_epochs: Number of training epochs\n",
        "\n",
        "    Returns:\n",
        "        model: Trained model\n",
        "        metrics: Evaluation metrics\n",
        "        model_name: Name of saved model\n",
        "    \"\"\"\n",
        "    import datetime\n",
        "    import json\n",
        "\n",
        "    print(f\"Training {task_type.upper()} dental age model:\")\n",
        "    print(f\"  - Backbone: {backbone}\")\n",
        "    print(f\"  - Data augmentation: {augment}\")\n",
        "    print(f\"  - Sex filter: {sex_filter if sex_filter else 'All'}\")\n",
        "\n",
        "    # Prepare data\n",
        "    train_loader, val_loader, train_df, val_df = prepare_data(\n",
        "        df_fixed, task_type=task_type, augment=augment,\n",
        "        sex_filter=sex_filter, backbone=backbone\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    use_attention = backbone == 'efficientnet'\n",
        "    model = DentalAgeModel(\n",
        "        task_type=task_type,\n",
        "        backbone=backbone,\n",
        "        use_attention=use_attention\n",
        "    ).to(device)\n",
        "\n",
        "    # Train model\n",
        "    model, train_losses, val_losses, perf_history = train_model(\n",
        "        model, train_loader, val_loader, task_type=task_type, num_epochs=num_epochs\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    metrics = evaluate_model(model, val_loader, task_type=task_type)\n",
        "\n",
        "    # Save model with metadata\n",
        "    model_name = save_model_with_metadata(\n",
        "        model, train_df, val_df, task_type, backbone, augment, sex_filter, metrics\n",
        "    )\n",
        "\n",
        "    # Visualize model interpretability\n",
        "    visualize_model_interpretability(model, val_df, task_type, backbone)\n",
        "\n",
        "    return model, metrics, model_name"
      ],
      "metadata": {
        "id": "YQsY5Iq4QJ3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Train and save models for different configurations\n",
        "# Binary classification with EfficientNet, with augmentation\n",
        "model_binary_eff, metrics_binary_eff, name_binary_eff = train_and_save_dental_model(\n",
        "    task_type='binary',\n",
        "    backbone='efficientnet',\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "# Binary classification with ViT, with augmentation\n",
        "model_binary_vit, metrics_binary_vit, name_binary_vit = train_and_save_dental_model(\n",
        "    task_type='binary',\n",
        "    backbone='vit',\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "# Regression with EfficientNet for males only\n",
        "model_regression_m, metrics_regression_m, name_regression_m = train_and_save_dental_model(\n",
        "    task_type='regression',\n",
        "    backbone='efficientnet',\n",
        "    sex_filter='M',\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "# Regression with EfficientNet for females only\n",
        "model_regression_f, metrics_regression_f, name_regression_f = train_and_save_dental_model(\n",
        "    task_type='regression',\n",
        "    backbone='efficientnet',\n",
        "    sex_filter='F',\n",
        "    augment=True\n",
        ")\n",
        "\n",
        "# List all saved models\n",
        "all_models = list_available_models()\n",
        "\n",
        "# Load a specific model\n",
        "model, config, train_df, val_df = load_model_with_metadata(name_binary_eff)\n",
        "\n",
        "# Load the most recent regression model for males\n",
        "model_m, config_m, train_df_m, val_df_m = load_model_with_metadata(\n",
        "    task_type='regression',\n",
        "    backbone='efficientnet',\n",
        "    sex_filter='M'\n",
        ")\n"
      ],
      "metadata": {
        "id": "tsyWzvVKQJyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train all remaining models\n",
        "\n",
        "# Function to check if model exists already\n",
        "def model_exists(task_type, backbone, sex_filter=None, augment=True):\n",
        "    sex_str = f\"_{sex_filter}\" if sex_filter else \"\"\n",
        "    aug_str = \"_aug\" if augment else \"\"\n",
        "    model_name = f\"{task_type}_{backbone}{sex_str}{aug_str}\"\n",
        "    model_dir = f\"/content/drive/MyDrive/DentalAgeClassification/{model_name}\"\n",
        "    return os.path.exists(model_dir)\n",
        "\n",
        "# 1. Complete the pooled dataset models\n",
        "task_types = ['binary', 'regression', 'multiclass']\n",
        "backbones = ['efficientnet', 'vit']\n",
        "augment_options = [True, False]\n",
        "\n",
        "for task_type in task_types:\n",
        "    for backbone in backbones:\n",
        "        for augment in augment_options:\n",
        "            # Skip models we already have\n",
        "            if model_exists(task_type, backbone, None, augment):\n",
        "                print(f\"Skipping existing model: {task_type}_{backbone}_{'aug' if augment else 'no_aug'}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nTraining pooled {task_type} model with {backbone} {'with' if augment else 'without'} augmentation\")\n",
        "            model, metrics, name = train_and_save_dental_model(\n",
        "                task_type=task_type,\n",
        "                backbone=backbone,\n",
        "                augment=augment\n",
        "            )\n",
        "\n",
        "# 2. Train sex-specific models\n",
        "sexes = ['M', 'F']\n",
        "\n",
        "for sex in sexes:\n",
        "    for task_type in task_types:\n",
        "        for backbone in backbones:\n",
        "            for augment in augment_options:\n",
        "                # Skip models we already have\n",
        "                if model_exists(task_type, backbone, sex, augment):\n",
        "                    print(f\"Skipping existing model: {task_type}_{backbone}_{sex}_{'aug' if augment else 'no_aug'}\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\nTraining {sex} {task_type} model with {backbone} {'with' if augment else 'without'} augmentation\")\n",
        "                model, metrics, name = train_and_save_dental_model(\n",
        "                    task_type=task_type,\n",
        "                    backbone=backbone,\n",
        "                    sex_filter=sex,\n",
        "                    augment=augment\n",
        "                )\n",
        "\n",
        "# List all trained models\n",
        "all_models = list_available_models()"
      ],
      "metadata": {
        "id": "61Yy1Q8UvMhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "def safe_str(value, width=10):\n",
        "    \"\"\"Safely convert any value to a string with padding\"\"\"\n",
        "    if value is None:\n",
        "        return \"N/A\".ljust(width)\n",
        "    try:\n",
        "        return str(value).ljust(width)\n",
        "    except:\n",
        "        return \"N/A\".ljust(width)\n",
        "\n",
        "def safe_format(value, format_spec=\".4f\", width=10):\n",
        "    \"\"\"Safely format a value that might be None or not a number\"\"\"\n",
        "    if value is None:\n",
        "        return \"N/A\".ljust(width)\n",
        "    try:\n",
        "        return f\"{float(value):{format_spec}}\".ljust(width)\n",
        "    except (ValueError, TypeError):\n",
        "        return str(value).ljust(width)\n",
        "\n",
        "def manual_compare_models(task_type=None, specific_models=None):\n",
        "    \"\"\"Compare model metrics by directly reading from model directories\"\"\"\n",
        "    base_dir = \"/content/drive/MyDrive/DentalAgeClassification\"\n",
        "    all_results = {}\n",
        "\n",
        "    # Collect model paths\n",
        "    if specific_models:\n",
        "        model_paths = [os.path.join(base_dir, model) for model in specific_models\n",
        "                       if os.path.exists(os.path.join(base_dir, model))]\n",
        "    else:\n",
        "        model_paths = [os.path.join(base_dir, d) for d in os.listdir(base_dir)\n",
        "                       if os.path.isdir(os.path.join(base_dir, d))]\n",
        "\n",
        "    # For each model, try to read metrics\n",
        "    for model_path in model_paths:\n",
        "        model_name = os.path.basename(model_path)\n",
        "        config_file = os.path.join(model_path, \"config.json\")\n",
        "\n",
        "        # Default values\n",
        "        results = {\n",
        "            'task_type': None,\n",
        "            'backbone': None,\n",
        "            'sex_filter': None,\n",
        "            'augment': None,\n",
        "            'accuracy': None,\n",
        "            'sensitivity': None,\n",
        "            'specificity': None,\n",
        "            'f1': None,\n",
        "            'roc_auc': None,\n",
        "            'mae': None,\n",
        "            'rmse': None,\n",
        "            'r2': None,\n",
        "            'binary_accuracy': None\n",
        "        }\n",
        "\n",
        "        # Try to infer properties from the model name\n",
        "        if 'binary' in model_name:\n",
        "            results['task_type'] = 'binary'\n",
        "        elif 'regression' in model_name:\n",
        "            results['task_type'] = 'regression'\n",
        "        elif 'multiclass' in model_name:\n",
        "            results['task_type'] = 'multiclass'\n",
        "\n",
        "        if 'efficientnet' in model_name:\n",
        "            results['backbone'] = 'efficientnet'\n",
        "        elif 'vit' in model_name:\n",
        "            results['backbone'] = 'vit'\n",
        "\n",
        "        if '_M_' in model_name or model_name.endswith('_M'):\n",
        "            results['sex_filter'] = 'M'\n",
        "        elif '_F_' in model_name or model_name.endswith('_F'):\n",
        "            results['sex_filter'] = 'F'\n",
        "        else:\n",
        "            results['sex_filter'] = 'All'\n",
        "\n",
        "        if '_aug' in model_name:\n",
        "            results['augment'] = True\n",
        "        else:\n",
        "            results['augment'] = False\n",
        "\n",
        "        # Try to load metrics from config.json\n",
        "        metrics_loaded = False\n",
        "\n",
        "        if os.path.exists(config_file):\n",
        "            try:\n",
        "                with open(config_file, 'r') as f:\n",
        "                    config = json.load(f)\n",
        "\n",
        "                # Extract properties from config\n",
        "                results['task_type'] = config.get('task_type', results['task_type'])\n",
        "                results['backbone'] = config.get('backbone', results['backbone'])\n",
        "                results['sex_filter'] = config.get('sex_filter', results['sex_filter'])\n",
        "                results['augment'] = config.get('augmentation', results['augment'])\n",
        "\n",
        "                # Extract metrics from nested 'metrics' key\n",
        "                metrics = config.get('metrics', {})\n",
        "\n",
        "                if metrics:\n",
        "                    # Binary classification metrics\n",
        "                    results['accuracy'] = metrics.get('accuracy')\n",
        "                    results['sensitivity'] = metrics.get('sensitivity')\n",
        "                    results['specificity'] = metrics.get('specificity')\n",
        "                    results['f1'] = metrics.get('f1')\n",
        "                    results['roc_auc'] = metrics.get('roc_auc')\n",
        "\n",
        "                    # Regression metrics\n",
        "                    results['mae'] = metrics.get('mae')\n",
        "                    results['rmse'] = metrics.get('rmse')\n",
        "                    results['r2'] = metrics.get('r2')\n",
        "                    results['binary_accuracy'] = metrics.get('binary_accuracy')\n",
        "\n",
        "                    metrics_loaded = True\n",
        "                    print(f\"Loaded metrics from config.json for {model_name}\")\n",
        "                else:\n",
        "                    print(f\"Config found for {model_name} but no metrics section\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading config.json for {model_name}: {e}\")\n",
        "\n",
        "        # Filter by task type if specified\n",
        "        if task_type and results['task_type'] != task_type:\n",
        "            continue\n",
        "\n",
        "        # Add to results if we have anything\n",
        "        if results['task_type']:\n",
        "            all_results[model_name] = results\n",
        "        else:\n",
        "            print(f\"Skipping {model_name} - No task type determined\")\n",
        "\n",
        "    if not all_results:\n",
        "        print(\"No models to compare after filtering\")\n",
        "        return {}\n",
        "\n",
        "    # Group by task type\n",
        "    task_groups = defaultdict(dict)\n",
        "    for name, metrics in all_results.items():\n",
        "        task = metrics['task_type']\n",
        "        if task:\n",
        "            task_groups[task][name] = metrics\n",
        "\n",
        "    # Generate comparisons for each task type\n",
        "    for task, models in task_groups.items():\n",
        "        if not models:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n\\n===== {task.upper()} MODELS =====\")\n",
        "        if task == 'binary':\n",
        "            compare_binary_models(models)\n",
        "        elif task == 'regression':\n",
        "            compare_regression_models(models)\n",
        "        elif task == 'multiclass':\n",
        "            compare_multiclass_models(models)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def compare_binary_models(models_metrics):\n",
        "    metrics_to_compare = ['accuracy', 'sensitivity', 'specificity', 'f1', 'roc_auc']\n",
        "    model_names = list(models_metrics.keys())\n",
        "\n",
        "    # Try to sort by accuracy, handle None values\n",
        "    def get_accuracy(model):\n",
        "        acc = models_metrics[model].get('accuracy')\n",
        "        try:\n",
        "            return float(acc) if acc is not None else -1\n",
        "        except:\n",
        "            return -1\n",
        "\n",
        "    model_names.sort(key=get_accuracy, reverse=True)\n",
        "\n",
        "    # Print table\n",
        "    print(\"\\nBinary Models Comparison:\")\n",
        "    header = f\"{'Model':<30} {'Backbone':<12} {'Sex':<8} {'Aug':<5} \" + \" \".join([f\"{m:<10}\" for m in metrics_to_compare])\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    for model in model_names:\n",
        "        metrics = models_metrics[model]\n",
        "        backbone = safe_str(metrics.get('backbone', 'unknown'), 12)\n",
        "        sex = safe_str(metrics.get('sex_filter', 'All'), 8)\n",
        "        aug = \"Yes\" if metrics.get('augment') else \"No \"\n",
        "\n",
        "        # Safe format values\n",
        "        vals = []\n",
        "        for metric in metrics_to_compare:\n",
        "            val = metrics.get(metric)\n",
        "            vals.append(safe_format(val))\n",
        "\n",
        "        vals_str = \" \".join(vals)\n",
        "        print(f\"{model:<30} {backbone} {sex} {aug:<5} {vals_str}\")\n",
        "\n",
        "    # Create plot with valid values only\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    x = np.arange(len(metrics_to_compare))\n",
        "    width = 0.8 / (len(model_names) or 1)  # Avoid division by zero\n",
        "\n",
        "    for i, model in enumerate(model_names):\n",
        "        # Get values, replacing None with 0\n",
        "        values = []\n",
        "        for metric in metrics_to_compare:\n",
        "            val = models_metrics[model].get(metric)\n",
        "            try:\n",
        "                values.append(float(val) if val is not None else 0)\n",
        "            except:\n",
        "                values.append(0)\n",
        "\n",
        "        offset = (i - len(model_names)/2 + 0.5) * width\n",
        "        plt.bar(x + offset, values, width, label=model, alpha=0.7)\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Binary Classification Models Comparison')\n",
        "    plt.xticks(x, metrics_to_compare)\n",
        "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3, fontsize='small')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"binary_models_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def compare_regression_models(models_metrics):\n",
        "    metrics_to_compare = ['mae', 'rmse', 'r2', 'binary_accuracy']\n",
        "    model_names = list(models_metrics.keys())\n",
        "\n",
        "    # Try to sort by MAE, handle None values\n",
        "    def get_mae(model):\n",
        "        mae = models_metrics[model].get('mae')\n",
        "        try:\n",
        "            return float(mae) if mae is not None else float('inf')\n",
        "        except:\n",
        "            return float('inf')\n",
        "\n",
        "    model_names.sort(key=get_mae)\n",
        "\n",
        "    # Print table\n",
        "    print(\"\\nRegression Models Comparison:\")\n",
        "    header = f\"{'Model':<30} {'Backbone':<12} {'Sex':<8} {'Aug':<5} \" + \" \".join([f\"{m:<10}\" for m in metrics_to_compare])\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    for model in model_names:\n",
        "        metrics = models_metrics[model]\n",
        "        backbone = safe_str(metrics.get('backbone', 'unknown'), 12)\n",
        "        sex = safe_str(metrics.get('sex_filter', 'All'), 8)\n",
        "        aug = \"Yes\" if metrics.get('augment') else \"No \"\n",
        "\n",
        "        # Safe format values\n",
        "        vals = []\n",
        "        for metric in metrics_to_compare:\n",
        "            val = metrics.get(metric)\n",
        "            vals.append(safe_format(val))\n",
        "\n",
        "        vals_str = \" \".join(vals)\n",
        "        print(f\"{model:<30} {backbone} {sex} {aug:<5} {vals_str}\")\n",
        "\n",
        "    # Plot with valid values only\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    x = np.arange(len(metrics_to_compare))\n",
        "    width = 0.8 / (len(model_names) or 1)  # Avoid division by zero\n",
        "\n",
        "    for i, model in enumerate(model_names):\n",
        "        values = []\n",
        "        for metric in metrics_to_compare:\n",
        "            val = models_metrics[model].get(metric)\n",
        "            try:\n",
        "                values.append(float(val) if val is not None else 0)\n",
        "            except:\n",
        "                values.append(0)\n",
        "\n",
        "        offset = (i - len(model_names)/2 + 0.5) * width\n",
        "        plt.bar(x + offset, values, width, label=model, alpha=0.7)\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Regression Models Comparison')\n",
        "    plt.xticks(x, metrics_to_compare)\n",
        "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3, fontsize='small')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"regression_models_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def compare_multiclass_models(models_metrics):\n",
        "    metrics_to_compare = ['accuracy', 'mae', 'rmse']\n",
        "    model_names = list(models_metrics.keys())\n",
        "\n",
        "    # Try to sort by accuracy, handle None values\n",
        "    def get_accuracy(model):\n",
        "        acc = models_metrics[model].get('accuracy')\n",
        "        try:\n",
        "            return float(acc) if acc is not None else -1\n",
        "        except:\n",
        "            return -1\n",
        "\n",
        "    model_names.sort(key=get_accuracy, reverse=True)\n",
        "\n",
        "    # Print table\n",
        "    print(\"\\nMulticlass Models Comparison:\")\n",
        "    header = f\"{'Model':<30} {'Backbone':<12} {'Sex':<8} {'Aug':<5} \" + \" \".join([f\"{m:<10}\" for m in metrics_to_compare])\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    for model in model_names:\n",
        "        metrics = models_metrics[model]\n",
        "        backbone = safe_str(metrics.get('backbone', 'unknown'), 12)\n",
        "        sex = safe_str(metrics.get('sex_filter', 'All'), 8)\n",
        "        aug = \"Yes\" if metrics.get('augment') else \"No \"\n",
        "\n",
        "        # Safe format values\n",
        "        vals = []\n",
        "        for metric in metrics_to_compare:\n",
        "            val = metrics.get(metric)\n",
        "            vals.append(safe_format(val))\n",
        "\n",
        "        vals_str = \" \".join(vals)\n",
        "        print(f\"{model:<30} {backbone} {sex} {aug:<5} {vals_str}\")\n",
        "\n",
        "    # Plot with valid values only\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    x = np.arange(len(metrics_to_compare))\n",
        "    width = 0.8 / (len(model_names) or 1)  # Avoid division by zero\n",
        "\n",
        "    for i, model in enumerate(model_names):\n",
        "        values = []\n",
        "        for metric in metrics_to_compare:\n",
        "            val = models_metrics[model].get(metric)\n",
        "            try:\n",
        "                values.append(float(val) if val is not None else 0)\n",
        "            except:\n",
        "                values.append(0)\n",
        "\n",
        "        offset = (i - len(model_names)/2 + 0.5) * width\n",
        "        plt.bar(x + offset, values, width, label=model, alpha=0.7)\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Multiclass Classification Models Comparison')\n",
        "    plt.xticks(x, metrics_to_compare)\n",
        "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3, fontsize='small')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"multiclass_models_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def summarize_best_models():\n",
        "    \"\"\"Generate a summary of the best models for each category\"\"\"\n",
        "    all_results = manual_compare_models()\n",
        "\n",
        "    print(\"\\n\\n===== SUMMARY OF BEST MODELS =====\")\n",
        "\n",
        "    # Find best binary model\n",
        "    binary_models = {m: metrics for m, metrics in all_results.items()\n",
        "                     if metrics.get('task_type') == 'binary'}\n",
        "\n",
        "    if binary_models:\n",
        "        best_binary = max(binary_models.items(),\n",
        "                          key=lambda x: float(x[1].get('accuracy', 0))\n",
        "                          if x[1].get('accuracy') is not None else -1)\n",
        "        print(f\"\\nBest Binary Classification Model: {best_binary[0]}\")\n",
        "        print(f\"  - Backbone: {safe_str(best_binary[1].get('backbone'))}\")\n",
        "        print(f\"  - Sex Filter: {safe_str(best_binary[1].get('sex_filter'))}\")\n",
        "        print(f\"  - Augmentation: {best_binary[1].get('augment')}\")\n",
        "        print(f\"  - Accuracy: {safe_format(best_binary[1].get('accuracy'))}\")\n",
        "        print(f\"  - Sensitivity: {safe_format(best_binary[1].get('sensitivity'))}\")\n",
        "        print(f\"  - Specificity: {safe_format(best_binary[1].get('specificity'))}\")\n",
        "        print(f\"  - F1 Score: {safe_format(best_binary[1].get('f1'))}\")\n",
        "        print(f\"  - ROC AUC: {safe_format(best_binary[1].get('roc_auc'))}\")\n",
        "\n",
        "    # Find best regression model\n",
        "    regression_models = {m: metrics for m, metrics in all_results.items()\n",
        "                         if metrics.get('task_type') == 'regression'}\n",
        "\n",
        "    if regression_models:\n",
        "        # Lower MAE is better\n",
        "        best_regression = min(regression_models.items(),\n",
        "                              key=lambda x: float(x[1].get('mae', float('inf')))\n",
        "                              if x[1].get('mae') is not None else float('inf'))\n",
        "        print(f\"\\nBest Regression Model: {best_regression[0]}\")\n",
        "        print(f\"  - Backbone: {safe_str(best_regression[1].get('backbone'))}\")\n",
        "        print(f\"  - Sex Filter: {safe_str(best_regression[1].get('sex_filter'))}\")\n",
        "        print(f\"  - Augmentation: {best_regression[1].get('augment')}\")\n",
        "        print(f\"  - MAE: {safe_format(best_regression[1].get('mae'))}\")\n",
        "        print(f\"  - RMSE: {safe_format(best_regression[1].get('rmse'))}\")\n",
        "        print(f\"  - R²: {safe_format(best_regression[1].get('r2'))}\")\n",
        "        print(f\"  - Binary Accuracy: {safe_format(best_regression[1].get('binary_accuracy'))}\")\n",
        "\n",
        "    # Find best multiclass model\n",
        "    multiclass_models = {m: metrics for m, metrics in all_results.items()\n",
        "                         if metrics.get('task_type') == 'multiclass'}\n",
        "\n",
        "    if multiclass_models:\n",
        "        best_multiclass = max(multiclass_models.items(),\n",
        "                              key=lambda x: float(x[1].get('accuracy', 0))\n",
        "                              if x[1].get('accuracy') is not None else -1)\n",
        "        print(f\"\\nBest Multiclass Model: {best_multiclass[0]}\")\n",
        "        print(f\"  - Backbone: {safe_str(best_multiclass[1].get('backbone'))}\")\n",
        "        print(f\"  - Sex Filter: {safe_str(best_multiclass[1].get('sex_filter'))}\")\n",
        "        print(f\"  - Augmentation: {best_multiclass[1].get('augment')}\")\n",
        "        print(f\"  - Accuracy: {safe_format(best_multiclass[1].get('accuracy'))}\")\n",
        "        print(f\"  - MAE: {safe_format(best_multiclass[1].get('mae'))}\")\n",
        "        print(f\"  - RMSE: {safe_format(best_multiclass[1].get('rmse'))}\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def run_all_comparisons():\n",
        "    \"\"\"Run all comparison functions\"\"\"\n",
        "    print(\"\\n\\n===== COMPARING ALL MODELS =====\")\n",
        "    all_results = manual_compare_models()\n",
        "\n",
        "    # By task type\n",
        "    print(\"\\n\\n===== BINARY MODELS ONLY =====\")\n",
        "    binary_results = manual_compare_models(task_type='binary')\n",
        "\n",
        "    print(\"\\n\\n===== REGRESSION MODELS ONLY =====\")\n",
        "    regression_results = manual_compare_models(task_type='regression')\n",
        "\n",
        "    print(\"\\n\\n===== MULTICLASS MODELS ONLY =====\")\n",
        "    multiclass_results = manual_compare_models(task_type='multiclass')\n",
        "\n",
        "    print(\"\\n\\n===== SUMMARY OF BEST MODELS =====\")\n",
        "    summarize_best_models()\n",
        "\n",
        "    return all_results"
      ],
      "metadata": {
        "id": "L2ZkvqdsQJoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_best_models()"
      ],
      "metadata": {
        "id": "gAfXpgapDl9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import json\n",
        "\n",
        "# Function to load a saved model and evaluate it\n",
        "def load_and_evaluate_model(model_dir):\n",
        "    \"\"\"\n",
        "    Load a previously trained model and evaluate it on validation data\n",
        "\n",
        "    Args:\n",
        "        model_dir: Path to the model directory\n",
        "\n",
        "    Returns:\n",
        "        model: The loaded model\n",
        "        metrics: Dictionary of evaluation metrics\n",
        "    \"\"\"\n",
        "    print(f\"Loading model from {model_dir}\")\n",
        "\n",
        "    # Load config\n",
        "    with open(os.path.join(model_dir, \"config.json\"), \"r\") as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    print(f\"Model type: {config['task_type']}\")\n",
        "    print(f\"Backbone: {config['backbone']}\")\n",
        "    print(f\"Sex filter: {config.get('sex_filter', 'None')}\")\n",
        "    print(f\"Augmentation used in training: {config.get('augmentation', False)}\")\n",
        "    print(f\"Creation date: {config.get('creation_date', 'Unknown')}\")\n",
        "\n",
        "    # Load validation data\n",
        "    val_df = pd.read_pickle(os.path.join(model_dir, \"val_df.pkl\"))\n",
        "    print(f\"Validation set size: {len(val_df)}\")\n",
        "\n",
        "    # Optional: Explore validation data\n",
        "    print(\"\\nValidation data sample:\")\n",
        "    print(val_df.head())\n",
        "\n",
        "    print(\"\\nValidation data statistics:\")\n",
        "    if config['task_type'] == 'binary':\n",
        "        # Use bin_label instead of label\n",
        "        print(val_df['bin_label'].value_counts())\n",
        "    else:\n",
        "        print(f\"Age range: {val_df['age'].min()} to {val_df['age'].max()}\")\n",
        "        print(f\"Sex distribution: {val_df['gender'].value_counts().to_dict()}\")\n",
        "\n",
        "    # Load the model\n",
        "    model_path = os.path.join(model_dir, \"model.pth\")\n",
        "\n",
        "    # Here you would load your model architecture based on config\n",
        "    # For simplicity, just showing the concept\n",
        "    if config['backbone'] == 'efficientnet':\n",
        "        # Load EfficientNet model\n",
        "        print(\"Loading EfficientNet model...\")\n",
        "        # model = YourEfficientNetArchitecture()\n",
        "    else:\n",
        "        # Load ViT model\n",
        "        print(\"Loading ViT model...\")\n",
        "        # model = YourViTArchitecture()\n",
        "\n",
        "    # Load weights\n",
        "    # model.load_state_dict(torch.load(model_path))\n",
        "    print(f\"Model weights loaded from {model_path}\")\n",
        "\n",
        "    # Load saved metrics\n",
        "    metrics = config.get('metrics', {})\n",
        "    print(\"\\nSaved performance metrics:\")\n",
        "    for key, value in metrics.items():\n",
        "        if isinstance(value, list):\n",
        "            print(f\"  {key}: {value}\")\n",
        "        else:\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "    # Optional: Using train data for comparison\n",
        "    train_df = pd.read_pickle(os.path.join(model_dir, \"train_df.pkl\"))\n",
        "    print(f\"\\nTraining set size: {len(train_df)}\")\n",
        "\n",
        "    # Compare training vs validation distributions\n",
        "    if config['task_type'] == 'binary':\n",
        "        train_dist = train_df['bin_label'].value_counts(normalize=True)\n",
        "        val_dist = val_df['bin_label'].value_counts(normalize=True)\n",
        "\n",
        "        print(\"\\nClass distribution comparison (Train vs Val):\")\n",
        "        for label in sorted(train_dist.index):\n",
        "            print(f\"  Class {label}: Train {train_dist[label]:.2f} - Val {val_dist[label]:.2f}\")\n",
        "\n",
        "    elif config['task_type'] in ['regression', 'multiclass']:\n",
        "        # Calculate age distribution\n",
        "        train_ages = train_df['age'].value_counts().sort_index()\n",
        "        val_ages = val_df['age'].value_counts().sort_index()\n",
        "\n",
        "        # Plot age distributions\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        train_ages.plot(kind='bar')\n",
        "        plt.title('Training Age Distribution')\n",
        "        plt.xlabel('Age')\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        val_ages.plot(kind='bar')\n",
        "        plt.title('Validation Age Distribution')\n",
        "        plt.xlabel('Age')\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Return loaded model and metrics\n",
        "    return metrics\n",
        "\n",
        "# Load the binary_efficientnet_aug model\n",
        "model_dir = \"/content/drive/MyDrive/DentalAgeClassification/binary_efficientnet_aug\"\n",
        "metrics = load_and_evaluate_model(model_dir)"
      ],
      "metadata": {
        "id": "97uXI-wcDlsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JzGeylrRDlgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, metrics = run_dental_age_analysis(task_type='binary', augment=True)"
      ],
      "metadata": {
        "id": "bShC5jlaovMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/Brazilian Dental Panoramic X-Ray Dataset.zip\""
      ],
      "metadata": {
        "id": "Z5SWUyePoCRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract the Brazilian dataset\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "zip_path = '/content/drive/MyDrive/Brazilian Dental Panoramic X-Ray Dataset.zip'\n",
        "local_zip = '/content/Brazilian_dataset.zip'\n",
        "local_extract = '/content'\n",
        "dataset_path = '/content/Brazilian_dataset'\n",
        "\n",
        "# Copy zip from Drive to Colab local disk\n",
        "if not os.path.exists(local_zip):\n",
        "    print(\"Copying Brazilian dataset from Drive to Colab local disk...\")\n",
        "    os.system(f'cp \"{zip_path}\" \"{local_zip}\"')\n",
        "else:\n",
        "    print(\"Zip file already exists locally\")\n",
        "\n",
        "# Extract if needed\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(\"Extracting Brazilian dataset...\")\n",
        "    os.system(f'unzip -oq \"{local_zip}\" -d \"{local_extract}\"')\n",
        "    # If the extraction creates a different folder structure, rename it\n",
        "    extracted_dirs = [d for d in os.listdir(local_extract)\n",
        "                     if os.path.isdir(os.path.join(local_extract, d)) and \"Brazilian\" in d]\n",
        "    if extracted_dirs:\n",
        "        os.system(f'mv \"{os.path.join(local_extract, extracted_dirs[0])}\" \"{dataset_path}\"')\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(\"Dataset already extracted\")\n",
        "\n",
        "print(f\"Brazilian dataset path: {dataset_path}\")"
      ],
      "metadata": {
        "id": "uZ2XOSoVowMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Brazilian dataset from CSVs and prepare DataFrame\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Dataset paths\n",
        "dataset_path = \"/content/Brazilian_dataset\"\n",
        "train_csv = os.path.join(dataset_path, \"train.csv\")\n",
        "val_csv = os.path.join(dataset_path, \"val.csv\")\n",
        "test_csv = os.path.join(dataset_path, \"test.csv\")\n",
        "\n",
        "# Load CSV files\n",
        "train_df = pd.read_csv(train_csv)\n",
        "val_df = pd.read_csv(val_csv)\n",
        "test_df = pd.read_csv(test_csv)\n",
        "\n",
        "# Combine all datasets\n",
        "all_df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
        "print(f\"Total samples in combined Brazilian dataset: {len(all_df)}\")\n",
        "\n",
        "# Convert file paths to absolute paths\n",
        "all_df['filepath'] = all_df['path'].apply(lambda x: os.path.join(dataset_path, x))\n",
        "\n",
        "# Round age to nearest integer\n",
        "all_df['age'] = np.round(all_df['age_in_years']).astype(int)\n",
        "\n",
        "# Filter for ages 14-24 only\n",
        "brazilian_df = all_df[(all_df['age'] >= 14) & (all_df['age'] <= 24)].copy()\n",
        "print(f\"Samples in age range 14-24: {len(brazilian_df)}\")\n",
        "\n",
        "# Add binary label and normalized age\n",
        "age_min, age_max = 14, 24\n",
        "brazilian_df['bin_label'] = (brazilian_df['age'] >= 18).astype(int)\n",
        "brazilian_df['age_norm'] = (brazilian_df['age'] - age_min) / (age_max - age_min)\n",
        "\n",
        "# Get age distribution by gender\n",
        "age_gender_counts = brazilian_df.groupby(['age', 'sex']).size().unstack(fill_value=0)\n",
        "age_gender_counts['Total'] = age_gender_counts.sum(axis=1)\n",
        "print(\"\\nAge and gender distribution:\")\n",
        "print(age_gender_counts)\n",
        "\n",
        "# Create a more formatted table for the manuscript\n",
        "table_data = []\n",
        "for age in sorted(brazilian_df['age'].unique()):\n",
        "    male_count = brazilian_df[(brazilian_df['age'] == age) & (brazilian_df['sex'] == 'M')].shape[0]\n",
        "    female_count = brazilian_df[(brazilian_df['age'] == age) & (brazilian_df['sex'] == 'F')].shape[0]\n",
        "    total_count = male_count + female_count\n",
        "    table_data.append([age, male_count, female_count, total_count])\n",
        "\n",
        "# Create DataFrame for the table\n",
        "table_df = pd.DataFrame(table_data, columns=['Age Range', 'M', 'F', 'Total'])\n",
        "\n",
        "# Calculate subtotals for minors and adults\n",
        "minors_male = brazilian_df[(brazilian_df['age'] < 18) & (brazilian_df['sex'] == 'M')].shape[0]\n",
        "minors_female = brazilian_df[(brazilian_df['age'] < 18) & (brazilian_df['sex'] == 'F')].shape[0]\n",
        "adults_male = brazilian_df[(brazilian_df['age'] >= 18) & (brazilian_df['sex'] == 'M')].shape[0]\n",
        "adults_female = brazilian_df[(brazilian_df['age'] >= 18) & (brazilian_df['sex'] == 'F')].shape[0]\n",
        "\n",
        "# Add subtotal rows\n",
        "subtotal_minors = pd.DataFrame([['Subtotal (<18)', minors_male, minors_female, minors_male + minors_female]],\n",
        "                              columns=['Age Range', 'M', 'F', 'Total'])\n",
        "subtotal_adults = pd.DataFrame([['Subtotal (≥18)', adults_male, adults_female, adults_male + adults_female]],\n",
        "                              columns=['Age Range', 'M', 'F', 'Total'])\n",
        "total_row = pd.DataFrame([['Total', brazilian_df[brazilian_df['sex'] == 'M'].shape[0],\n",
        "                          brazilian_df[brazilian_df['sex'] == 'F'].shape[0], brazilian_df.shape[0]]],\n",
        "                        columns=['Age Range', 'M', 'F', 'Total'])\n",
        "\n",
        "# Combine all parts of the table\n",
        "final_table = pd.concat([table_df, subtotal_minors, subtotal_adults, total_row])\n",
        "\n",
        "print(\"\\nTable for manuscript:\")\n",
        "print(final_table)\n",
        "\n",
        "# Create a nicely formatted markdown table\n",
        "print(\"\\nMarkdown table for manuscript:\")\n",
        "print(\"| Age Range | M | F | Total |\")\n",
        "print(\"|-----------|---|---|-------|\")\n",
        "for _, row in table_df.iterrows():\n",
        "    print(f\"| {row['Age Range']}-{row['Age Range']}.99 | {row['M']} | {row['F']} | {row['Total']} |\")\n",
        "print(f\"| Subtotal (<18) | {minors_male} | {minors_female} | {minors_male + minors_female} |\")\n",
        "print(f\"| Subtotal (≥18) | {adults_male} | {adults_female} | {adults_male + adults_female} |\")\n",
        "print(f\"| Total | {brazilian_df[brazilian_df['sex'] == 'M'].shape[0]} | {brazilian_df[brazilian_df['sex'] == 'F'].shape[0]} | {brazilian_df.shape[0]} |\")"
      ],
      "metadata": {
        "id": "39ma0D4p_XZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sample images from the Brazilian dataset\n",
        "def visualize_sample_images(df, num_samples=3):\n",
        "    if len(df) == 0:\n",
        "        print(\"No samples to visualize!\")\n",
        "        return\n",
        "\n",
        "    # Sample stratified by age if possible\n",
        "    if len(df) > num_samples:\n",
        "        samples = df.groupby('age', group_keys=False).apply(\n",
        "            lambda x: x.sample(min(1, len(x)))\n",
        "        ).sample(num_samples, replace=len(df.age.unique()) < num_samples)\n",
        "    else:\n",
        "        samples = df.sample(min(num_samples, len(df)))\n",
        "\n",
        "    # Display images\n",
        "    plt.figure(figsize=(15, 5*num_samples))\n",
        "    for i, (_, row) in enumerate(samples.iterrows()):\n",
        "        try:\n",
        "            plt.subplot(num_samples, 1, i+1)\n",
        "            img = Image.open(row['filepath']).convert('RGB')\n",
        "            plt.imshow(img)\n",
        "            plt.title(f\"Age: {row['age']}, Sex: {row['sex']}\")\n",
        "            plt.axis('off')\n",
        "        except Exception as e:\n",
        "            print(f\"Error displaying image {row['filepath']}: {e}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"brazilian_samples.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Visualize data transformations\n",
        "def visualize_brazilian_transformations():\n",
        "    if len(brazilian_df) == 0:\n",
        "        print(\"No samples available for transformation visualization!\")\n",
        "        return\n",
        "\n",
        "    # Get a sample image from each major age group if possible\n",
        "    sample_groups = [(14, 17), (18, 21), (22, 24)]\n",
        "    sample_images = []\n",
        "\n",
        "    for min_age, max_age in sample_groups:\n",
        "        group_samples = brazilian_df[(brazilian_df['age'] >= min_age) & (brazilian_df['age'] <= max_age)]\n",
        "        if len(group_samples) > 0:\n",
        "            sample_images.append(group_samples.iloc[0]['filepath'])\n",
        "\n",
        "    if not sample_images:\n",
        "        print(\"No suitable images found for transformation visualization\")\n",
        "        return\n",
        "\n",
        "    # Visualize transformations on the first valid sample\n",
        "    visualize_transformations(sample_images[0])\n",
        "\n",
        "# Execute visualizations\n",
        "print(\"Visualizing sample images from Brazilian dataset:\")\n",
        "visualize_sample_images(brazilian_df)\n",
        "\n",
        "print(\"\\nVisualizing transformations on Brazilian dataset images:\")\n",
        "visualize_brazilian_transformations()"
      ],
      "metadata": {
        "id": "I3Rkzw6XvBEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for evaluating models on the Brazilian dataset\n",
        "def evaluate_on_brazilian_dataset(model, task_type='binary', backbone='efficientnet'):\n",
        "    \"\"\"\n",
        "    Evaluate a pre-trained model on the Brazilian dataset\n",
        "\n",
        "    Args:\n",
        "        model: The trained model to evaluate\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "        backbone: 'efficientnet' or 'vit'\n",
        "    \"\"\"\n",
        "    # Create validation transform for this backbone\n",
        "    _, val_transform = get_transforms(augment=False, backbone=backbone)\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    brazilian_dataset = DentalAgeDataset(brazilian_df, val_transform, task_type)\n",
        "    brazilian_loader = DataLoader(brazilian_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"\\n===== Evaluating {backbone.upper()} {task_type.capitalize()} Model on Brazilian Dataset =====\")\n",
        "    print(f\"Total samples: {len(brazilian_dataset)}\")\n",
        "\n",
        "    # Evaluate model\n",
        "    metrics = evaluate_model(model, brazilian_loader, task_type)\n",
        "\n",
        "    # Visualize results\n",
        "    if task_type == 'binary':\n",
        "        visualize_binary_results(metrics)\n",
        "    elif task_type == 'multiclass':\n",
        "        visualize_multiclass_results(metrics)\n",
        "    else:  # regression\n",
        "        visualize_regression_results(metrics)\n",
        "\n",
        "    # Visualize model interpretability on select samples\n",
        "    visualize_model_interpretability_on_brazilian(model, task_type, backbone)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def visualize_model_interpretability_on_brazilian(model, task_type='binary', backbone='efficientnet'):\n",
        "    \"\"\"Visualize model interpretability on Brazilian samples\"\"\"\n",
        "    # Get 3 samples from different age groups if possible\n",
        "    sample_groups = [(14, 17, \"Minor\"), (18, 21, \"Young Adult\"), (22, 24, \"Adult\")]\n",
        "    sample_df = pd.DataFrame()\n",
        "\n",
        "    for min_age, max_age, _ in sample_groups:\n",
        "        group_samples = brazilian_df[(brazilian_df['age'] >= min_age) & (brazilian_df['age'] <= max_age)]\n",
        "        if len(group_samples) > 0:\n",
        "            sample_df = pd.concat([sample_df, group_samples.sample(min(1, len(group_samples)))],\n",
        "                                 ignore_index=True)\n",
        "\n",
        "    if len(sample_df) == 0:\n",
        "        print(\"No suitable samples found for interpretability visualization\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n===== Visualizing {backbone.upper()} Model Interpretability on Brazilian Samples =====\")\n",
        "    visualize_model_interpretability(model, sample_df, task_type, backbone)"
      ],
      "metadata": {
        "id": "l5pBh4a5u5tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained models from disk\n",
        "def load_trained_model(task_type='binary', backbone='efficientnet'):\n",
        "    \"\"\"\n",
        "    Load a pre-trained dental age model from disk\n",
        "\n",
        "    Args:\n",
        "        task_type: 'binary', 'multiclass', or 'regression'\n",
        "        backbone: 'efficientnet' or 'vit'\n",
        "    \"\"\"\n",
        "    model_path = f\"./models/DentalAge_{task_type}_{backbone}/best_model.pth\"\n",
        "\n",
        "    # Create model instance\n",
        "    use_attention = backbone == 'efficientnet'  # Only use attention for EfficientNet\n",
        "    model = DentalAgeModel(task_type=task_type, backbone=backbone, use_attention=use_attention).to(device)\n",
        "\n",
        "    # Load weights\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(f\"Successfully loaded {task_type} {backbone} model weights\")\n",
        "        model.eval()\n",
        "        return model\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Model file not found at {model_path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "VNUveMmtwfOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and evaluate binary classification models\n",
        "print(\"\\nEvaluating Binary Classification Models on Brazilian Dataset\")\n",
        "\n",
        "# Load EfficientNet binary model\n",
        "binary_model_eff = load_trained_model(task_type='binary', backbone='efficientnet')\n",
        "\n",
        "# Evaluate on Brazilian dataset\n",
        "if binary_model_eff is not None:\n",
        "    binary_metrics_eff = evaluate_on_brazilian_dataset(\n",
        "        binary_model_eff, task_type='binary', backbone='efficientnet'\n",
        "    )\n",
        "\n",
        "# Load ViT binary model\n",
        "binary_model_vit = load_trained_model(task_type='binary', backbone='vit')\n",
        "\n",
        "# Evaluate on Brazilian dataset\n",
        "if binary_model_vit is not None:\n",
        "    binary_metrics_vit = evaluate_on_brazilian_dataset(\n",
        "        binary_model_vit, task_type='binary', backbone='vit'\n",
        "    )\n",
        "\n",
        "# Compare binary models on Brazilian dataset\n",
        "if 'binary_metrics_eff' in locals() and 'binary_metrics_vit' in locals():\n",
        "    print(\"\\n===== Comparing Binary Models on Brazilian Dataset =====\")\n",
        "    metrics_names = ['Accuracy', 'Sensitivity', 'Specificity', 'ROC AUC']\n",
        "\n",
        "    eff_values = [\n",
        "        binary_metrics_eff.get('accuracy', 0),\n",
        "        binary_metrics_eff.get('sensitivity', 0),\n",
        "        binary_metrics_eff.get('specificity', 0),\n",
        "        binary_metrics_eff.get('roc_auc', 0)\n",
        "    ]\n",
        "\n",
        "    vit_values = [\n",
        "        binary_metrics_vit.get('accuracy', 0),\n",
        "        binary_metrics_vit.get('sensitivity', 0),\n",
        "        binary_metrics_vit.get('specificity', 0),\n",
        "        binary_metrics_vit.get('roc_auc', 0)\n",
        "    ]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    x = np.arange(len(metrics_names))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, eff_values, width, label='EfficientNet', alpha=0.7)\n",
        "    plt.bar(x + width/2, vit_values, width, label='Vision Transformer', alpha=0.7)\n",
        "\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Binary Classification: EfficientNet vs Vision Transformer on Brazilian Data')\n",
        "    plt.xticks(x, metrics_names)\n",
        "    plt.legend()\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.ylim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"brazilian_binary_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Lpgxto0GwfJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, config, train_df, val_df = load_model_with_metadata(\n",
        "    model_name=\"binary_efficientnet_aug\"\n",
        ")"
      ],
      "metadata": {
        "id": "wvpSMlbJwsLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model_interpretability(model, val_df, task_type='binary', backbone='efficientnet')"
      ],
      "metadata": {
        "id": "COm1dB0elQ7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, config, train_df, val_df = load_model_with_metadata(\n",
        "    model_name=\"binary_vit_aug\"\n",
        ")"
      ],
      "metadata": {
        "id": "eIBP-cnGnhQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model_interpretability(model, val_df, task_type='binary', backbone='vit')"
      ],
      "metadata": {
        "id": "9rtGimzipg3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 3rd Revision Code\n",
        "# Evaluate regression + multiclass using saved val_df.pkl\n",
        "# Generates scatter plots, confusion matrices, etc.\n",
        "# ================================\n",
        "\n",
        "model_names = [\n",
        "    \"multiclass_efficientnet_aug\",\n",
        "    \"multiclass_vit_aug\",\n",
        "    \"regression_efficientnet_aug\",\n",
        "    \"regression_vit_aug\"\n",
        "]\n",
        "\n",
        "def infer_task_type(name):\n",
        "    if \"multiclass\" in name:\n",
        "        return \"multiclass\"\n",
        "    if \"regression\" in name:\n",
        "        return \"regression\"\n",
        "    raise ValueError(\"Unknown task type in name\")\n",
        "\n",
        "def infer_backbone(name):\n",
        "    return \"vit\" if \"vit\" in name.lower() else \"efficientnet\"\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(\"\\n==============================\")\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "    # Load model, config, TRAIN_DF and VAL_DF saved during training\n",
        "    model, config, train_df, val_df = load_model_with_metadata(model_name)\n",
        "\n",
        "    task_type = infer_task_type(model_name)\n",
        "    backbone  = infer_backbone(model_name)\n",
        "\n",
        "    print(f\"Using saved validation set with {len(val_df)} samples\")\n",
        "\n",
        "    # -----------------------------------\n",
        "    # BUILD VAL DATASET FROM SAVED val_df.pkl\n",
        "    # -----------------------------------\n",
        "    _, val_transform = get_transforms(augment=False, backbone=backbone)\n",
        "    val_dataset = DentalAgeDataset(val_df, val_transform, task_type=task_type)\n",
        "    val_loader  = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    # -----------------------------------\n",
        "    # EVALUATE MODEL (USING YOUR evaluate_model FUNCTION)\n",
        "    # -----------------------------------\n",
        "    print(f\"Evaluating {model_name} ({task_type}, {backbone})\")\n",
        "    metrics = evaluate_model(model, val_loader, task_type=task_type)\n",
        "    all_results[model_name] = metrics\n",
        "\n",
        "    # -----------------------------------\n",
        "    # GENERATE REQUIRED PLOTS FOR REVIEWER #2\n",
        "    # -----------------------------------\n",
        "    print(f\"Generating required plots for {model_name}\")\n",
        "\n",
        "    if task_type == \"multiclass\":\n",
        "        visualize_multiclass_results(metrics)\n",
        "\n",
        "    if task_type == \"regression\":\n",
        "        # This now works because metrics[\"predictions\"] and metrics[\"targets\"] exist\n",
        "        visualize_regression_results(metrics)\n",
        "\n",
        "print(\"\\nDone. All regression + multiclass plots generated correctly.\")\n"
      ],
      "metadata": {
        "id": "DrBDg7tgpmqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wvFAJqluUgPR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}